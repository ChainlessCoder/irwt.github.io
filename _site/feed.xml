<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lum Ramabaja</title>
    <description>Welcome to my blog</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 12 Nov 2020 19:28:11 +0100</pubDate>
    <lastBuildDate>Thu, 12 Nov 2020 19:28:11 +0100</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Scaling Blockchains (Part 1) - The Interactive Bloom Proof</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;When talking about the “scalability” of blockchains, it’s always good to define first what exactly we mean by it. In this article, I will not focus on things like “how many transactions can be processed by a chain per unit of time”. Instead, I am more interested to explore how we can scale the number of nodes in the system, while still keeping it decentralized and trustless. This problem might sound as an easy one to solve, but as you will see, it’s actually quite a tricky problem.&lt;/p&gt;

&lt;p&gt;The structure of this article will be like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, I will describe some fundamental concepts that one has to know before understanding the problem and the solution proposed by this article. I will describe what &lt;strong&gt;full nodes&lt;/strong&gt; are, what &lt;strong&gt;Simplified Payment Verification (SPV)&lt;/strong&gt; is, how it works, and how &lt;strong&gt;Merkle trees&lt;/strong&gt; are used in blockchains.&lt;/li&gt;
  &lt;li&gt;In the second part of the article, I’ll explain the two different implementations of SPV. I will also explain the shortcomings of both variants. Note: Eventhough other chains, such as Ethereum, use Patricia trees, instead of Merkle trees, and use an account-based model, instead of an UTXO model, the same issues hold true for Ethereum as well.&lt;/li&gt;
  &lt;li&gt;In the last part of the article, I’ll  introduce a possible solution to the scalability shortcomings of SPV. Here I will introduce the idea of an Interactive Bloom Proof (IBP), an interesting concept I came up with while researching at &lt;a href=&quot;bloomlab.io&quot;&gt;Bloom Lab&lt;/a&gt;. It allows for decentralized, truly trustless SPV-like interactions (something that, as we will see, is not possible today), while at the same time allowing the network to scale easily. One can actually build a whole new blockchain architecture around the IBP concept, but that’s a topic for another time. As implementing the Interactive Bloom Proof into existing chains requires hard forking them, I do not expect this idea to get much traction. But I think it is a very useful concept to people working on new blockchain designs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;full-nodes-and-simplified-payment-verification&quot;&gt;Full Nodes and Simplified Payment Verification&lt;/h2&gt;
&lt;p&gt;A device that connects to a blockchain network is referred to in the blockchain space as a “node”. Nodes can be classified as either “full nodes”, or “lightweight nodes”. Full nodes store every block and transaction of a blockchain and constantly check the validity of incoming messages. Full nodes independently create a chain by following some consensus rules. In the case of Bitcoin and (as of this writing) Ethereum, &lt;strong&gt;Proof of Work (PoW)&lt;/strong&gt; is the consensus algorithm used for creating the chain, aka decentralized ledger. Consensus algorithms are quite a large topic and deserve a whole article for themselves. As for this article, if you are not already familiar with PoW, just keep in mind that it’s the algorithm that guarantees that every full node will end up with the same locally stored chain. In other words, thanks to PoW and the chain’s validity rules, full nodes will converge to the same blockchain state, without having to trust anybody. That said, not many users use full nodes to access the blockchain however. Due to the large storage capacity blockchains require (in the case of Bitcoin, as of this writing one needs approximately 302 Gb, and Ethereum’s “archive nodes” more than 4 Tb) most users opt-in with lighweight nodes, or some centralized service.&lt;/p&gt;

&lt;p&gt;Lightweight nodes rely on something known as &lt;strong&gt;simplified payment verification&lt;/strong&gt; (SPV). SPV was initially proposed in the original Bitcoin whitepaper and is quite straightforward as a concept:
Instead of making nodes download every block in a blockchain, a lightweight node downloads only the block headers. That way, instead of having to store every transaction, one simply stores the previous block hash, a nonce (a value needed for PoW, and a &lt;strong&gt;Merkle root&lt;/strong&gt; (a specific hash). This alone cuts the storage costs significantly. Having locally stored the block headers of a chain, a lightweight node can easily prove if a transaction really occurred by requesting a &lt;strong&gt;Merkle proof&lt;/strong&gt;, also known as a &lt;strong&gt;Merkle path&lt;/strong&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;700&quot; height=&quot;700&quot; src=&quot;/assets/images/posts/2020/spv.png&quot; /&gt;
  &lt;em&gt;&lt;br /&gt;Figure 1. An illustration of how block headers are structured. Taken from the Bitcoin whitepaper.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;To understand what exactly a Merkle root and a Merkle proof really are, we have to know how a &lt;strong&gt;Merkle tree&lt;/strong&gt; works. A Merkle tree is a binary tree in which all leaf nodes (i.e. the Merkle tree’s elements, i.e. the block’s transactions) are associated with a cryptographic hash, and all none-leaf nodes are associated with a cryptographic hash, that is formed from the hashes of its child nodes, as shown in figure two.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;450&quot; height=&quot;450&quot; src=&quot;/assets/images/posts/2020/mt.jpg&quot; /&gt;
  &lt;em&gt;&lt;br /&gt;Figure 2. An illustration of Merkle tree. The leaf nodes represent a block's transactions. After the hashing process, a root hash is generated, also known as a Merkle root.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;Now when a lightweight node wants to know if a transaction really occurred, instead of having to request all block transactions to generate the block hash, one simply needs a subset of the hashes, as shown in orange in figure 3. Having those hashes, and having the block headers stored locally, a lightweight node can recreate the Merkle root and thus know if a transaction really occurred.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;450&quot; height=&quot;450&quot; src=&quot;/assets/images/posts/2020/mp.jpg&quot; /&gt;
  &lt;em&gt;&lt;br /&gt;Figure 3. An illustration of Merkle tree and a Merkle proof (shown in orange) for a given transaction hash (shown in blue).&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;And that’s all simplified payment verification really is. It’s a neat idea that allows the casual user still to participate in a blockchain network, without having to download hundreds of gigabytes and validate every transaction for themselves. There is though a catch that makes today’s SPV less useful than people might think.&lt;/p&gt;

&lt;h2 id=&quot;the-shortcomings-of-spv&quot;&gt;The shortcomings of SPV&lt;/h2&gt;
&lt;p&gt;Back in 2019 there was a lot of fuss on Twitter around chapter 8 of Bitcoin’s whitepaper, the chapter that introduces the concept of SPV. It turned out that different people define the implementation of SPV differently. For convenience, for figure 4., I am going to use Donald Mulders’ illustration from his &lt;a href=&quot;https://metanet.id/simplified-payment-verification-spv/&quot;&gt;blog&lt;/a&gt; post to clarify the difference.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;450&quot; height=&quot;450&quot; src=&quot;/assets/images/posts/2020/spv_variants.jpg&quot; /&gt;
  &lt;em&gt;&lt;br /&gt;Figure 4. Two ways how SPV can be implemented.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;The left model visualizes how most SPV implementations work today. Alice sends a transaction to the blockchain network, it gets included to the chain, Bob can verify that it was included, and he can see that other blocks are added on top of Alice’s transaction. Bob cannot validate the transaction for himself, but as long as the honest nodes represent the majority in the network, he can know that Alice’s transaction was validated and inserted to the chain by the network’s full nodes. Unlike Craig Wright’s claims, it’s quite clear that the left model was originally described in the Bitcoin Whitepaper. Below is a screenshot of the part of the paper for you to read and compare it for yourself with the two models shown in figure 4.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;450&quot; height=&quot;450&quot; src=&quot;/assets/images/posts/2020/spv_whitepaper_screenshot.jpg&quot; /&gt;
  &lt;em&gt;&lt;br /&gt;Figure 5. Screenshot of chapter 8 of the Bitcoin whitepaper.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;The second model (the model on the right of figure 4) is the model that Craig says is what he really meant when “writing the Bitcoin whitepaper”. Here, instead of Alice having to be connected to the blockchain network, she can simply spend her transaction directly with Bob. Bob can then send the transaction to the network and check if it’s valid. Before criticizing this model, I first want to mention it’s advantages: In this model, Alice does not have to be connected to the network at all to spend her coins. This is quite a huge advantage in my opinion. She can simply send the transaction, as well as the necessary Merkle paths to Bob directly, she doesn’t even have to have an internet connection necessarily. Bob can then check if the right Merkle root of a block header can be created from Alice’s spent transaction(s) and provide the service / product to Alice right away. Bob can then send the transaction to the network for it to be included in the chain. He is in fact incentivized to transmit it to the network as soon as possible, as Alice might spend the same coins later somewhere else.&lt;/p&gt;

&lt;p&gt;But wait, according to the model in figure four, there’s also a verification step Bob has to perform with the blockchain network. Well, and that’s where the catch with the right model is. In SPV, even if you get a correct Merkle proof that “locks” a specific transaction with a block header, there’s no way for the lightweight node to know if that transaction was already spend or not. To clarify this last statement a bit, imagine let’s imagine a scenario with Alice and Bob again. Alice received some bitcoin at block number 1000. She then spent her coins at block 1010 for some reason. Now let’s say Bob is a lightweight node following Craigh’s model from figure 4 (the one at the right). Alice could fool Bob by sending him the Merkle proof of the already spent coins from block 1000. Bob would receive the proof, see that Alice really had that many coins at block 1000, and accept it as true. SPV can only show that a transaction really occurred in the chain, it cannot prove that it is still unspent. That’s why in this model, Bob has to communicate with a number of nodes and get their validation that the transaction is unspent. This however is done in a trustful way, i.e. Bob has to contact a bunch of full nodes and hope that the majority will be honest. Note, this is only the case if the chain is based on a &lt;a href=&quot;https://en.wikipedia.org/wiki/Unspent_transaction_output&quot;&gt;UTXO&lt;/a&gt; model, an &lt;a href=&quot;https://blockonomi.com/utxo-vs-account-based-transaction-models/&quot;&gt;account-based&lt;/a&gt; model, like in Ethereum, Bob could get the proof of Alice’s latest account state. Craigh’s model then is not really &lt;strong&gt;trustless&lt;/strong&gt;, and of course he knows that. That’s why he will often say things such as “It is important to note that network miners act within the law” and “It is so because Bitcoin is not designed to be a system that acts outside of the controls of law”. This kind of thinking though doesn’t really align well with the “crypto ethos”. It might just be me, but I don’t really see what’s the point of having all the cryptographic intermediary steps, if at the end of the day you still need to trust full nodes in a non-cryptographic way.&lt;/p&gt;

&lt;p&gt;Now that this is out of the way, let’s focus on the interesting things: Thee scaling issues of the original SPV (the left one in figure 4, i.e. the one that was really presented in the Bitcoin whitepaper). Jameson Lopp has a great &lt;a href=&quot;https://www.coindesk.com/spv-support-billion-bitcoin-users-sizing-scaling-claim&quot;&gt;article&lt;/a&gt; about SPV’s scaling issues. The fundamental problem boils down to this - SPV are very efficient for lightweight nodes, but not for the full nodes. As quoted from Lopp’s article:&lt;/p&gt;

&lt;p&gt;“Every SPV client must sync the entire blockchain from the last time it had contact with the network, or, if it believes it missed transactions, it will need to rescan the entire blockchain since the wallet creation date. In the worst case, at time of writing, this is approximately 150GB. The full node must load every single block from disk, filter it to the client’s specifications and return the result”.&lt;/p&gt;

&lt;p&gt;Ergo, there’s a top ceiling (quite a low one in fact) for how many lightweight nodes can be in the system. Please refer to Lopp’s article for concrete numbers. The fact of the matter is, that with today’s architecture setup, full nodes can simply not serve the increasing number of lightweight nodes. It’s not only a matter of incentives (which is also a problem, but one that is easier to solve), it’s a matter of connectivity. The network of full nodes would simply get clogged. Since lightweight nodes can further be lied to by omission, they have to contact multiple full nodes, making the scalability issue even worse.&lt;/p&gt;

&lt;p&gt;To promote easily user onboarding, a concept like that of SPV is crucial. Not everyone can, or even wants to run a full node. We need thus a system that preserves the convenience of SPV, but removes the load from full nodes. Having such a setup would allow a blockchain network to grow in size with fewer network restrictions. But for this we’ll need a whole new architecture, and that’s where the Interactive Bloom Proof comes in.&lt;/p&gt;

&lt;h1 id=&quot;the-interactive-bloom-proof-ibp&quot;&gt;The Interactive Bloom Proof (IBP)&lt;/h1&gt;

&lt;h2 id=&quot;full-nodes-20&quot;&gt;Full Nodes 2.0&lt;/h2&gt;
&lt;p&gt;I mentioned Craigh’s model before, because it’s an interesting design, even if it has its flaws. If lightweight nodes could interact with other lightweight nodes, and prove that some coins really belong to someone (as in SPV), while at the same time knowing that the coins are still unspent (something not possible in SPV), that would solve parts of the scalability issue. Before being able to explain the necessary changes we have to make to the blocks of a blockchain for this idea, we’ll first have to know how a &lt;a href=&quot;https://github.com/labbloom/bloom-tree&quot;&gt;Bloom tree&lt;/a&gt; works. For anyone interested to know more about the details of the Bloom tree, I’d recommend reading the original &lt;a href=&quot;https://arxiv.org/pdf/2002.03057.pdf&quot;&gt;paper&lt;/a&gt;. It’s a simple concept, but one has to be familiar with both bloom Filters and Merkle trees. If you’re not familiar with bloom filters, please read the first page of the paper. The Bloom tree has an interesting property: It allows for probabilistic presence proofs and non-probabilistic absence proofs in a bandwidth efficient and secure way. In other words, just as in Bloom filters, a Bloom tree can either tell us that an element &lt;strong&gt;might&lt;/strong&gt; be in a set (i.e. false positives are possible), or that it definitely is not in the set (i.e. false negatives are not possible). As we will see, the false positive nature of Bloom filters will not hinder us in any way.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;400&quot; height=&quot;400&quot; src=&quot;/assets/images/posts/2020/bl1.jpg&quot; /&gt;
  &lt;em&gt;&lt;br /&gt;Figure 6. Illustration of how a bloom filter is populated. An element is hashed k times, and the appropriate indices get switched from zero to one.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;For the IBP to work, whenever full nodes build a new block, they also build an empty Bloom filter with predefined parameters and populate it with every transaction that was spent in the given block.
In the case of the unspent transaction output model (UTXO model), older transactions get unlocked and consumed to create a new transaction. While a block’s Merkle tree gets constructed through the newly created UTXOs, this Bloom filter gets populated by the transactions that just got consumed.
Unlike regular Bloom filters, the way we populate this Bloom filter is a bit different: Instead of hashing every spent transaction hash &lt;em&gt;k&lt;/em&gt; times, we compute &lt;strong&gt;h(t,c)&lt;/strong&gt; &lt;em&gt;k&lt;/em&gt; times, where &lt;em&gt;h&lt;/em&gt; can be a non-cryptographic hash function like &lt;a href=&quot;https://en.wikipedia.org/wiki/MurmurHash&quot;&gt;murmur&lt;/a&gt;, &lt;em&gt;t&lt;/em&gt; is the transaction hash, and &lt;em&gt;c&lt;/em&gt; is a counter. I will get back to this counter in a moment. Once we have populated the Bloom filter with all the spent block transactions, we can build a Merkle tree on top of it. This is essentially what I am referring to as a Bloom Tree. The &lt;a href=&quot;https://github.com/labbloom/bloom-tree&quot;&gt;github&lt;/a&gt; implementation divides the Bloom filter into 64 bit chunks, which then are repeatedly hashed to create the tree. It also uses a &lt;a href=&quot;https://arxiv.org/pdf/2002.07648.pdf&quot;&gt;Compact Merkle Multiproof&lt;/a&gt; for more space efficient Merkle proofs.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;450&quot; height=&quot;450&quot; src=&quot;/assets/images/posts/2020/bt_1.jpg&quot; /&gt;
  &lt;em&gt;&lt;br /&gt;Figure 6. Illustration of a Bloom tree.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;With this simple structure, we can provide absence proofs (i.e. show that a transaction did no occur in a block), or probabilistic presence proofs (i.e. show that a transaction might have occurred in a block). For e detailed explanation of the proofs, please refer to the paper, it’s quite an easy read. Now getting back to that counter used to populate the Bloom filter - We will actually not create only one Bloom Tree for a block, but &lt;em&gt;N&lt;/em&gt; Bloom trees, each with a different counter, from 0 to &lt;em&gt;N&lt;/em&gt; (this will make sense in a moment). This will result in &lt;em&gt;N&lt;/em&gt; Bloom filters, each independent of each other, and each with a different mapping (i.e. the bits of the bit array differ for each of the &lt;em&gt;N&lt;/em&gt; Bloom filters). The roots of the Bloom trees are then added to the block header (or depending how large &lt;em&gt;N&lt;/em&gt; is set, a conventional Merkle tree can be built on top of the Bloom tree roots, and we can insert that Merkle root instead). We can then use PoW to insert the block into the chain, and the Bloom filters can be discarded. To recap, a block header in this setup has all the same components as before (even a regular Merkle root built from the block transactions), but it will also have a Merkle root built from the &lt;em&gt;N&lt;/em&gt; Bloom tree roots, which were built from the &lt;em&gt;N&lt;/em&gt; Bloom filters, which were populated with the block transactions. The size of &lt;em&gt;N&lt;/em&gt; will depend on the parameters of the bloom filters (&lt;em&gt;m&lt;/em&gt;, &lt;em&gt;n&lt;/em&gt;,&lt;em&gt;k&lt;/em&gt;), we will get back to this a bit later.&lt;/p&gt;

&lt;h2 id=&quot;bloom-nodes&quot;&gt;Bloom Nodes&lt;/h2&gt;
&lt;p&gt;Ok, up to this point, most of the full node 2.0 setup does not make much sense from the perspective of a full node. Lot’s of computations that slow down block validation, ridiculous. But it enables us to come up with a new node type that allows full nodes to focus on what they’re good at, mining. Let’s call this new node type a Bloom node. You can imagine Bloom nodes as a type of node that is in size between full nodes, and lightweight nodes. Like lightweight nodes, they store the block headers of a chain, instead of the whole blocks. But additionally to that, for every block header, a Bloom node will also store one of the block’s &lt;em&gt;N&lt;/em&gt; bloom filters, as well as a Merkle proof that connects the Bloom tree root (which we can compute with the stored bloom filter) with the block headers Merkle root. Which bloom filter gets chosen by a node, gets determined from the Bloom nodes address. If for example Node_address % &lt;em&gt;N&lt;/em&gt; returns 4, then that Bloom node will store the Bloom filters that have a counter &lt;em&gt;c&lt;/em&gt; equal to four. We do this, because as we will see in a bit, it will be important for the Bloom nodes to know which of its peers has which Bloom filters stored. Now that we have a picture of how the Bloom node is organized, let’s look at an example for how it would operate:&lt;/p&gt;

&lt;p&gt;Let’s take a scenario as in figure four (right model). Alice sends Bob directly a coin, as well as a Merkle proof showing that the coin truly belongs to her and occurred in the chain. Bob will check the specified block header and he will see that the coin truly belongs to Alice. Now though, Bob wants to know if the coin is still unspent, without having to contact and trust some full nodes. Bob hashes the UTXO’s consumed inputs &lt;em&gt;k&lt;/em&gt; times, and starting from the block header Alice’s coin was referenced at, performs these step iteratively until the latest block header is reached:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Go to the next block header&lt;/li&gt;
  &lt;li&gt;Take the bloom filter that is stored for that block&lt;/li&gt;
  &lt;li&gt;Check the state of the bloom filter at the &lt;em&gt;k&lt;/em&gt; indices&lt;/li&gt;
  &lt;li&gt;If at least one of the bits is zero, you know that Alice’s coin was not spent in this block.&lt;/li&gt;
  &lt;li&gt;If all the bits of the bloom filter are set to zero, Alice’s coin might have been spent at this block. Append the block height to a list.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once Bob iterates over his block headers, Bob will end up with a list of block heights in which Alice’s coin might have been spent. Bob can then ask his peers, who are also Bloom nodes, for absence proofs for those block heights. If all the block heights receive an absence proof, Bob can know for a fact that Alice’s coin was not spent in the meanwhile, and that it really belongs to her. Bob can then send the transaction to full nodes, so that it can be inserted to the transaction pools of the full node network.&lt;/p&gt;

&lt;p&gt;If for a specific block height, Bob receives presence proofs from all &lt;em&gt;N&lt;/em&gt; contacted peers (each peer would be contacted for a different bloom filter), then Bob can know for a fact that Alice’s coin was already spent. Here it is important to mention that &lt;em&gt;N&lt;/em&gt; will ideally should be chosen in a way that results in an astronomically small probability for a false positive. For example, If the false positive rate of a single bloom filter was set to be 0.001, then by having &lt;em&gt;N&lt;/em&gt; set to 25, one would have a total false positive rate of &lt;code class=&quot;highlighter-rouge&quot;&gt;1 x 10^75&lt;/code&gt;. Thus, if we were to populate a bloom filter with around &lt;code class=&quot;highlighter-rouge&quot;&gt;3000&lt;/code&gt; transactions, the size of the bloom filter would be only around &lt;code class=&quot;highlighter-rouge&quot;&gt;43133&lt;/code&gt; bits (feel free to play around with this Bloom filter &lt;a href=&quot;https://hur.st/bloomfilter/?n=3000&amp;amp;p=0.001&amp;amp;m=&amp;amp;k=&quot;&gt;calculator&lt;/a&gt;), so around one magnitude fewer bits than storing a whole block. By increasing &lt;em&gt;N&lt;/em&gt;, one can decrease &lt;em&gt;m&lt;/em&gt; further (the size of the Bloom filter). One can of course not increase &lt;em&gt;N&lt;/em&gt; indefinitely, as a smaller &lt;em&gt;m&lt;/em&gt; means a higher false positive rate for a single Bloom filter, which means having a larger list of block heights that need to be checked with neighboring Bloom nodes.&lt;/p&gt;

&lt;p&gt;Something very interesting should be noted here: If a transaction is not spent, proving its “unspentness” will be relatively cheap. We need only a single absence proof from a Bloom filter to know that a transaction was not spent. If on the other hand, a transaction was truly spent and Alice was lying, than Bob would have to collect &lt;em&gt;N&lt;/em&gt; presence proofs from his peers, which can be expensive. I will provide a solution to this in the Discussions section, as it’s quite easy to solve in an efficient way. But I will also show why this is actually a feature, not a “bug”, from a scalability perspective.&lt;/p&gt;

&lt;h1 id=&quot;a-pub-sub-for-bloom-nodes&quot;&gt;A ‘Pub Sub’ for Bloom Nodes&lt;/h1&gt;
&lt;p&gt;One of the good things about the above mentioned procedure, is that Bloom nodes do not have to ask full nodes for the block headers, or even bloom filters. Full nodes could send the block headers + Bloom filters to the Bloom node network, who then can get the block headers + corresponding Bloom filters from one-another. Since Bloom nodes can know that a Bloom filter was part of a block via a Merkle proof, they would simply follow the longest chain rule in a trustless way. At the beginning of the IBP section however, I mentioned that IBP “would solve parts of the scalability issue”. It’s true that Bloom nodes do not have to trust and ask several full nodes to know if a transaction is unspent, but they will still have to periodically contact full nodes to get their UTXOs and the Merkle proofs for those UTXO. Remember, When Alice sent the transaction to Bob, she sent it with a Merkle proof. She will still need to get that information from full nodes. As long as Bloom nodes still have to contact full nodes periodically, the scalability issue remains.&lt;/p&gt;

&lt;p&gt;We can overcome this issue however, by using a &lt;a href=&quot;https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf&quot;&gt;Kademlia&lt;/a&gt; like distributed hash table (DHT) to store the latest UTXOs within the Bloom node network. Whenever a new UTXO gets created, we can store it, together with the UTXOs Merkle proof in the Bloom nodes’ DHT network. The holder’s address could be used as a key, whereas the UTXO and the Merkle proof could be used as values. When Alice comes online, she can simply ask the network for any new UTXOs sent to her address. Spent transactions could be periodically deleted from the network, so that the Bloom node network does not have to store too many additional things.&lt;/p&gt;

&lt;p&gt;This way Bloom nodes would not have to contact full nodes at all, allowing for significant scaling. Full nodes would serve as publishers to the Bloom nodes DHT network, while Bloom nodes could “subscribe” to their addresses (or other addresses as well).&lt;/p&gt;

&lt;h1 id=&quot;discussions&quot;&gt;Discussions&lt;/h1&gt;
&lt;p&gt;The problem of having to collect a lot of presence proofs, if a transaction was already spent, is solvable. After explaining how one can solve it however, I will also explain why it might be better to keep it that way. Instead of collecting &lt;em&gt;N&lt;/em&gt; presence proofs from the Bloom trees, we could collect a single Merkle proof from the conventional Merkle tree that is used in blocks (the one that is built from the transactions). Done. This way we would require a single Merkle proof for both absence proofs and presence proofs. I don’t find this solution elegant enough however. To get such a presence proof, a Bloom node would have to contact the full nodes network (because the Bloom nodes DHT network would not contain that Merkle proof). We don’t want that.&lt;/p&gt;

&lt;p&gt;To prevent a malicious Bloom node to DDoS the Bloom nodes network, we could add rules to remove a peer from one’s peer list, if the peer sends invalid transactions. Another way to prevent malicious Bloom nodes to DDoS the network, would be by forcing the transaction sender to collect the absence / presence proofs, instead of the receiver. There are various ways one can deal with this issue.&lt;/p&gt;

&lt;p&gt;Overall, as mentioned at the beginning of the article, I do not expect this idea to get much traction. Hard forking existing chains is always a tricky thing, plus a lot of the modern chains use account-based models. The currently presented IBP proposal would have to be modified to work with an account-based chain (which isn’t that difficult, but again, hard forks suck). I do think though that the Interactive Bloom Proof idea in combination with a pub sub like network, has a lot of potential. I think that we’re still in the early-stage for blockchain architectures. There are still a lot of problems in the current way we build blockchains that need solving. The IBP might be a useful tool in a blockchain engineer’s / researcher’s toolbox.&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Oct 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/scaling-blockchains-part1-ibp</link>
        <guid isPermaLink="true">http://localhost:4000/scaling-blockchains-part1-ibp</guid>
        
        <category>Blockchain</category>
        
        <category>Bloom Filter</category>
        
        <category>SPV</category>
        
        
      </item>
    
      <item>
        <title>The Engram</title>
        <description>&lt;p&gt;This post is part of an ongoing online book. To access the other parts, please refer to the contents page of the &lt;a href=&quot;https://lums.blog/The-Competitive-Neuron&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The brain is unbelievably complex. Walking in a park, having a conversation with your friends, understanding what you’re reading, hearing, being aware of others’ feelings, these are enormously complex processes. They might seem trivial to us, but when looking at them from a machine’s perspective, we can see how impossibly difficult they really are. All that is somehow governed by a huge cluster of tiny cells in our heads, also known as neurons. A neuron in itself does not ‘‘know’’ anything, it is just a biological machine, but somehow a bunch of neurons together form a consciousness. This, is one of the greatest unsolved mysteries of our time.&lt;/p&gt;

&lt;p&gt;It seems like the mind is an emergent property of the brain. “Emergence” is when a system has properties which its parts do not have, but emerge because of rules, or interactions between its parts. Let’s take an ant colony as an example. An ant is pretty dumb. Its decision making abilities are limited, and individually it can’t plan anything ahead. Yet an ant colony is enormously complex and smart. Ants in a colony have different “jobs”, perform various collective tasks, and live in self-made structures that are way too complex for the mental capability of a single ant. One could arguably call an ant colony an organism in itself. It can make complex decisions as if it were a single entity, yet under the hood it’s all governed by a decentralised system of tiny, almost brainless ants. Our brains are not much different in that regard. The brain is a collection of dumb machines, that somehow together do very smart things.&lt;/p&gt;

&lt;p&gt;Parts in an emergent system often follow very simple rules. When those parts interact with one another, complexity can arise. During the evolution of the nervous system, many mental algorithms emerged, each from simple neuronal interactions, and each giving organisms a fascinating ability. The ability to form memories, to imagine, to infer the actions of another organism, all these abilities are derived from emergent algorithms that “run on top” of neural circuits. When talking about ‘‘algorithms’’, we should not imagine a software running on a computer. Even a vending machine can be viewed as an algorithm. You put coins into the machine, select an item, and it will perform a specific sequence of steps. Procedures, that’s all what algorithms are. In the case of the mind, the neural circuits in our head are what enable such procedures to emerge.&lt;/p&gt;

&lt;p&gt;Memory, induction, the ability to detect movement, the ability to detect the  passage of time, are all emergent algorithms. Even more complex processes, such as deduction, creativity, or imagination fall in the same realm, all these processes somehow emerge from neural circuits. Changes in these neural circuits or neural structures, can result in changes to an algorithm, or in some cases even create a new algorithm. If a useful algorithm appears due to a mutation in one of the neural circuits, that organism will have a competitive advantage over other organisms. Evolutionary forces can then shape the circuits and the neuronal structures over time, in a way that makes the emergent algorithm more efficient.&lt;/p&gt;

&lt;p&gt;This is precisely why I argue that it is crucial to understand the biology of the brain. If an emergent algorithm significantly helps an organism survive, and as a result enables the organism to have on average more offspring, then any organism with a more optimised circuitry  for the emergent algorithm, will have a bigger advantage over other organisms. This might sound over complicated at first, but what this means, is that over a long time, evolutionary forces will shape the underlying neural circuitry in a way that optimises for the emergent algorithms. This means that it should be possible to reverse engineer the emergent algorithms, by looking at the optimised physical structures and biochemical processes of neural circuits.&lt;/p&gt;

&lt;p&gt;Always remember: In biology, form follows function. If there is a specific pattern in nature, that repeats over and over again in different species, we can be very confident that the repeating pattern has an important role. By looking at the optimised morphology, structures, and processes of different neurons, we can start to guess their purpose in the context of the mind. Notice how I am not saying “in the context of learning”, but “in the context of the mind”. The ability to learn, i.e. the ability to infer correctly future events based on past experiences, is only one emergent phenomenon in a set of phenomena to which I am referring to as “the mind”. To decipher how the mind works, I argue that we have to treat it as a set of emergent algorithms. The ability to learn, deduce, imagine, memorise, all these processes are emergent algorithms that are part of the mind. I want to emphasise one thing however: I am not claiming that we can understand all emergent-phenomena of the brain by looking at individual neurons. Even if we know a neuron inside out, it doesn’t tell us much about how a group of neurons interact with one-another. A reductionist mindset will not bring us far when trying to decipher the mind. By treating the brain on the other hand as a complex system with several emergent properties, we can start to guess how different parts interconnect with one-another.&lt;/p&gt;

&lt;p&gt;In this book, I am particularly interested to examine the algorithm that gives rise to memory. When I say “memory”, I do not only mean the ability to recall past events. What I mean by it, is the ability to store external, or behavioural information in neurons and neural circuits. Neuroscientists often use different terms for memory, such as explicit memory, or implicit memory, depending on the kind of memory we are talking about. In this book any information that gets stored in neurons, or neural circuits, will simply be referred to as “memory”. Do not get deceived by the term however, memory in the context of the brain is very different from the memory we use in computers. Today’s computers are based on something known as the von Neumann architecture, where we have a CPU (the basic processing unit), and a memory. In this setup, for any kind of processing, the memory has to be moved from the memory unit to the processing unit. This is time consuming and quite energy inefficient. The setup for processing and memory in the brain on the other hand is much more sophisticated. Computing and memory are not distinctly separated in the brain, they go hand in hand. somehow they are co-located in the neural circuits. It is truly mind blowing when you think about it - somehow the mesh of neurons located in our skull, is able to rapidly store external information and also retrieve it shockingly fast and accurately. To this day, we have no idea how the brain does it, but we are getting closer! We know for example that a memory tends to be associated with specific activation of neurons. Each of our experiences and behaviours somehow gets associated with a neural pattern.&lt;/p&gt;

&lt;p&gt;When you smell a flower for example, a specific pattern of neurons in your brain will fire together, giving you that specific sensation. Once that pattern appears, a cascade of other patterns can emerge. The odour of the flower might trigger a specific memory. You might remember a meadow that you once visited as a child. An emotion might emerge in the cascade of patterns, causing you to feel nostalgia. All this because of a single neural pattern that was caused by the odour of a flower. How was that pattern formed? The question is not only about the physical substrate of memory (which we will examine in much detail), but about the coordination of read and write operations as well. How does the brain know how to form patterns, and how does the brain know which patterns to activate in which scenarios, and at which time? Unlike computers, the real brain  is highly asynchronous.&lt;/p&gt;

&lt;p&gt;“Asynchrony” is when events in a system do not occur in a sequential order, but rather during overlapping time periods. We say that something runs “concurrently” when events, or computations can advance without waiting for all other events, or computations to complete. In the case of the brain, the bursts of firing neurons represents the concurrent events. How can brain activities converge into specific neural patterns, when the whole system runs concurrently? If a neuron fires just a bit too early, or a bit too late than the other neurons, the desired pattern might not emerge, which could for example lead to an execution failure of an action down the line. As an example, you might see a ball flying at your direction, fail to process the right action due to a bad timing between neurons in your brain, and get hit by it in the face. The fact that such scenarios do not happen all the time, makes this all the more intriguing. The problem of asynchronous communication in neural circuits is especially tricky, because none of the neurons “know” when the other neurons will fire, but somehow they still manage to coordinate and act as one. Any computer scientists can tell you how difficult it is to build a working, robust asynchronous system. It is in fact a nightmare. And yet here we are, each one of us carrying with us the most complex asynchronous machine in the known universe, and it spends less energy than a light bulb.&lt;/p&gt;

&lt;p&gt;The problem of neural coordination can be categorised as a Byzantine Generals Problem. The Byzantine Generals Problem was first named by Lamport, Shostak, and Pease in a wonderful paper in 1982, which not surprisingly was titled ‘‘&lt;a href=&quot;https://people.eecs.berkeley.edu/~luca/cs174/byzantine.pdf&quot;&gt;The Byzantine Generals Problem&lt;/a&gt;’’. The abstract idea of it is simple: Imagine that you are a general in the byzantine army and are planning to attack an enemy fortress. The fortress is completely surrounded by several of your battalions, each controlled by a different general. If you all manage to attack the fortress at the same time, you will succeed in taking it down. An uncoordinated attack on the other hand, will end in defeat. How do you make sure that all battalions will attack at the same moment? Even though the Byzantine Generals Problem was originally conceived and formalised as a condition in distributed computing systems, we can see how neural circuits share a very similar problem. neurons are more likely to fire when they receive their inputs simultaneously. It seems like right timing is a necessary component for proper pattern formation and activation. As we will see later in later chapters, the temporal aspect of the neural coordination problem plays a key role in memory formation.&lt;/p&gt;

&lt;p&gt;Besides emerging from completely asynchronous processes, memory has also another very interesting property - It is “re-programmable”. Different organisms will have different memories, depending on their experiences. You were not born knowing what an apple is, but once you look at an apple, once you taste one, you will store a mental representation of the apple in your brain. Even more fascinating, next time you see an apple, you will know what it is, even though the apple you had the first time looked slightly different from the new one. The ability to figure out what something is, based on past experiences is also known as inductive reasoning, and it proved to be a very useful tool in the evolution of animals.&lt;/p&gt;

&lt;p&gt;Besides re-programmable memory, living organisms also have hard-coded memories, which evolved out of necessity. Ants for example (and many other social insects) have an interesting imprinted behaviour known as necrophoresis. Whenever there are any dead bodies of other members in the colony nest, or in the highways where the ants travel, the ants will carry the corpses and put them in a pile that is far enough from the colony. Ants never learned this kind of behaviour, they were born with it. In other words, this behaviour is pre-programmed in their genetic code, it did not emerge through experience. We can find even more interesting cases of hard-coded behaviours in honeybees. Honeybees communicate the location of nearby flowers with dance. They perform either “waggle dances”, or “round dances” inside their beehive, in the presence of other bees. The other bees receive the information from the dance, and then fly to the specific flower location. The preciseness of the honeybee language is astonishing, what is even more surprising however, is that it is encoded in their DNA. Different species of bees will have different “dialects” for communicating with one-another. Back in 1995, T. E. RindererL. D. Beaman &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/24169907/&quot;&gt;showed&lt;/a&gt; how certain dance dialects differences followed simple Mendelian rules of inheritance.&lt;/p&gt;

&lt;p&gt;Even us humans have encoded memories. Mammalian infants for example can smell and instinctively reach the mother’s breast after birth. Human babies are even able to automatically open their mouth when something is near their mouth, or to start sucking when something is in their mouth. This kind of behaviour was not taught, the information for it was already encoded in the brain - we were born with it. However, only the most crucial of behaviours can be encoded genetically. Adopting new behaviours by encoding memories in the genetic code of a population’s gene pool is a very slow and inconvenient process for most tasks. A mobile organism will need to assess more situations than a static one, and because of that, it will need to have a much larger pool of  possible behaviours to pick from. Coding all that into a genome becomes quickly infeasible. Infants for example are not born with the ability to see, or perform complex movements, as both these tasks are so complex, that they need to be developed through experience.&lt;/p&gt;

&lt;p&gt;The first multicellular mobile organisms thus needed some kind of hardware module, not only with imprinted procedures but with re-programmable memory as well. Mobile organisms needed a way to store new experiences and behaviours on the go. A key word here is “mobile”. A lot of scientists in fact believe that movement is why organisms evolved brains in the first place. In the race for survival, mobile organisms had to evolve the ability for ever more adaptive and complex movements and survival strategies. This became only possible once re-programmable memory evolved. As in previous biological systems, the hardware and algorithms of these organisms were pre-determined by their DNA. The “software” (or the memory) on the other hand was able to change dynamically within an organism’s lifetime. This was a game changer.  Suddenly the complexity of the behaviours an organism could perform was not bound by the slow, genetic instructions that they carried around with them. Organisms were for the first time able to perform quick adjustments to their own software.&lt;/p&gt;

&lt;p&gt;Amusingly enough, without mobility, the need for a brain also disappears. An interesting example for this is the intriguing life-cycle of the sea squirt. This animal, at the beginning of its life-cycle, lives as tadpole-like larvae with a developed nervous system and the ability to swim. Since the larvae is not capable of feeding itself, it will try to find a nice rock and cement itself on it, where it will live for the rest of its life. Once it settles like this, a fascinating transformation unfolds. The larvae starts to digest all its tadpole-like parts, including its rudimentary little brain, to transform into a developed sea squirt. Since it does not move anymore, it does not need a “brain” to live. There is however also another very interesting link between mobility and re-programmable memory.&lt;/p&gt;

&lt;p&gt;It might sound strange, but our sensory systems also require re-programmable memory.  Let’s take the visual system as an example. For our neurons to be able to make sense of the world, they need to be able to detect edges, shapes, colors and depth from the visual input. That kind of information has to be encoded, i.e. stored somehow by the neurons. That’s where the re-programmable memory comes in. Note again that when using the word “memory”, I am referring to the ability to store external, or behavioral information somehow into neural circuits. Thus for the development of vision, memory is required. What’s even stranger though, is that the feedback from movement is just as important. As long as an organism is immobile, it will not be able to learn how to see. Richard Held and Alan Hein made this clear in a fascinating experiment in 1963 known as the “Kitten Carousel”.&lt;/p&gt;

&lt;p&gt;The researchers placed two very young kittens into a carousel that was ringed in vertical stripes. The kittens were still too young to have a developed visual system. The first kitten was able to walk freely inside the carousel according to its own actions, while the second kitten was riding in a gondola inside the carousel, without the ability to control its movement. Both kittens would receive the same kind of visual stimuli from the attached vertical stripes when moving in the carousel. One would expect the two kittens to develop normal vision, but that was not the case. Only the kitten that was able to walk freely developed normal vision. The kitten that did not move but rode the gondola, never managed to learn how to see properly. Its visual system did not undergo any significant changes like that of the first kitten. It seems like vision is more than just receiving inputs to our eyes. To learn how to see, one requires feedback loops from other signals that an organism generates, such as movement. The end result of all the signals and feedbacks is the encoded memory that enables the ability of vision.&lt;/p&gt;

&lt;p&gt;We are only now starting to understand how the mechanisms for such “re-programmable memory” work. This might sound a bit far-fetched, but we are in fact in a very similar position to where Gregor Mendel, the father of modern genetics was almost two centuries ago, when he proposed his famous unit of inheritance (today known as the gene). Mendel was the first person to notice predictable patterns in the inheritance of traits. After thousands of experiments, back in 1865, Mendel proposed a hypothetical unit for inheriting traits. He called these heredity units “factor”. Back then, people knew that different traits were inheritable, but very few people however bothered to ask &lt;em&gt;how&lt;/em&gt; traits were inherited. It is important to remember that back then, Mendel had absolutely no idea what genes were, what their physical substrate was, or how they worked. These hereditary units were purely hypothetical, but they could explain very well his experimental observations.&lt;/p&gt;

&lt;p&gt;Today, a lot of scientists believe in another such hypothetical unit, a unit not for traits, but for memories. We refer to those hypothetical units as &lt;em&gt;Engrams&lt;/em&gt;, or memory-traces. Mendel’s hypothetical unit served as a storage medium for inheritance, while the hypothetical Engrams serve as a storage medium for cognitive information. Its exact mechanisms and the locations of the Engrams however remain enigmatic. We do know some things at least. Remember how different experiences, such as odours, could activate different patterns of neurons in the brain. We know that assemblies of neurons in our brain are somehow associated with specific experiences. Whenever an experience occurs, the neurons of the associated pattern fire together. Donald Hebb, a famous neuropsychologist, noticed this strange phenomenon, and proposed a marvellous theory in his 1949 book ‘‘&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-70911-1_15&quot;&gt;The Organization of Behaviour&lt;/a&gt;’’. Today Hebb’s proposal is known as Hebbian Theory, or Hebb’s rule and it explains how assemblies of neurons can become Engrams. Hebb stated that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;'’Any two cells or systems of cells that are repeatedly active at the same time will tend to become ‘associated’ so that activity in one facilitates activity in the other.’’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In laymen terms, Hebb’s rule states that “neurons that fire together, wire together”. That over simplified statement is indeed what we observe. If an input to the brain (such as an odour) causes the same pattern of neurons to activate repeaditly together, that pattern will become interassociated - it will start to fire together. Many scientists believe that this pattern “auto-association” is the perfect candidate for the engram. There is now overwhelming evidence that ensembles of neurons play an important role in memory storage. A fascinating &lt;a href=&quot;https://www.cell.com/cell/fulltext/S0092-86741930616-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867419306166%3Fshowall%3Dtrue&quot;&gt;experiment&lt;/a&gt; in 2019 for example, showed how by artificially activating behaviourally relevant ensembles of neurons, one could observe consistent behavioural responses from test animals. In other words, whenever the specific behavioural pattern was artificially activated, the mice performed the behaviour that was associated with the pattern. To achieve the forced pattern activation, the researchers genetically engineered the mice to have neurons covered with special proteins, that when hit by a light-beam cause the neuron to fire. This ingenious technique is also known as optogenetics. What was even more fascinating however, was that some ensembles of neurons had pattern completion properties. By simply artificially activating two of the ensemble neurons, the researchers were able to trigger the entire ensemble to activate. The results of the experiment beautifully coincide with Hebb’s rule.&lt;/p&gt;

&lt;p&gt;In the scientific community, the biological process behind Hebb’s theory is also known as “Spike-timing-dependent plasticity” (STDP). If neuron A activates neuron B, spike-timing-dependent plasticity is the process that adjusts the connection strength between the two neurons, depending on the timing of the activations. Hebb’s statement however, that “Any two cells that are repeatedly active at the same time will tend to become ‘associated’ so that activity in one facilitates activity in the other”, is an observation, not a mechanism. Why do two neurons become active at the same time in the first place? What exactly is happening at a cellular level when neurons fire? Might the process of engram formation actually be more complicated than in the proposed spike-timing-dependent plasticity model? This is what I will aim to answer in the upcoming chapters. We will learn how engrams are really formed and stored.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/the-engram</link>
        <guid isPermaLink="true">http://localhost:4000/the-engram</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>The Competitive Neuron - On the Formation of Memories and Neuronal Specialisation</title>
        <description>&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;h2 id=&quot;introduction&quot;&gt;&lt;a href=&quot;https://lums.blog/introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;part-one---biology&quot;&gt;Part One - Biology&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lums.blog/the-engram&quot;&gt;The Engram&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lums.blog/the-ghost-in-the-neuron&quot;&gt;The Ghost In The Neuron&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;part-two---technology&quot;&gt;Part Two - Technology&lt;/h2&gt;
</description>
        <pubDate>Sun, 03 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/The-Competitive-Neuron</link>
        <guid isPermaLink="true">http://localhost:4000/The-Competitive-Neuron</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>Introduction</title>
        <description>&lt;p&gt;This post is part of an ongoing online book. To access the other parts, please refer to the contents page of the &lt;a href=&quot;https://lums.blog/The-Competitive-Neuron&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Beyond Ghor, there was a city. All its inhabitants were blind. A king with his entourage arrived nearby; he brought his army and camped in the desert. He had a mighty elephant, which he used to increase the people’s awe. The populace became anxious to see the elephant, and some sightless from among this blind community ran like fools to find it. As they did not even know the form or shape of the elephant, they groped sightlessly, gathering information by touching some part of it. Each thought that he knew something, because he could feel a part…. The man whose hand had reached an ear… said: “It is a large, rough thing, wide and broad, like a rug.” And the one who had felt the trunk said: “I have the real facts about it. It is like a straight and hollow pipe, awful and destructive.”. The one who had felt its feet and legs said: “It is mighty and firm, like a pillar.” Each had felt one part out of many. Each had perceived it wrongly….&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Idries Shah - Tales of the Dervishes, 1967&lt;/cite&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This ancient story was told to teach a simple lesson that is often ignored: The behaviour of a system cannot be known just by knowing the elements of which the system is made.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We have come a long way since  Santiago Ramón y Cajal published his first iconic drawings of neurons back in the late 19th century. The sheer amount of discoveries made by scientists since then proves that we are gradually converging on an understanding of how the brain works. The ingenious techniques that neuroscientists are developing to record and analyse our brains, are now helping us illuminate one by one parts of a once unknown world. Whereas the neuronal morphology and the molecular mechanisms of different neural structures are relatively well understood, the bigger picture of how neuronal interactions form emergent phenomena remains enigmatic. More than a century after the neuron’s discovery, we still do not know how the neural circuity in our heads gives rise to the mind. Like the blind men beyond Ghor, we are still trying to make sense of the elephant ourselves.&lt;/p&gt;

&lt;p&gt;Humans in fact tried to understand the nature of the mind for centuries. In ancient India for example, philosophers believed in a theory called “Samskara”. Samskara meant different things to different people, and it was always somehow mixed with religion and mysticism. It represented the mental impressions, or psychological imprints of a person. Samskaras were explained as characteristics, or behavioural traits that one either possessed from the moment of birth, or that got shaped over time. Indian philosophers  of the Nyaya school of Hinduism understood that a newborn child has imprinted memories (even though they did not refer to it like that). They argued that a baby’s instinctive reach for the mother’s breast was a sign that the baby had some prior Samskara. Since no one provided the knowledge of the necessity of the mother’s breast to the baby, and since the baby did not form any samskaras so far, philosophers believed that the newborn’s knowledge came from a ‘‘prior experience’’.&lt;/p&gt;

&lt;p&gt;As always, it was the Ancient Greeks however that hit the nail on its head. Alcmaeon of Croton, one of the greatest minds of Ancient Greece, was the first one to propose that the brain is the organ of the mind. Even though this revelation might not sound like a big deal today, back then it was a revolution in human knowledge. Up until then, it was not that obvious to people that the thinking happens in the brain. The brain was just another organ. Even the father of modern science, Democritus of Abdera, who formulated the atomic theory of the universe, was inspired by Alcmaeon’s discovery. Democritus concurred with Alcmaeon’s discovery and argued that perception is a purely mechanistic (or one might say algorithmic) process. He argued that thinking and feeling were simply features of matter that emerge when organised in a sufficiently fine and complex way and not due to some spirit infused into matter by the gods. During the time of Democritus, where everything was fused with spirituality and mysticism, these were not only bold statements, they were world-shattering. We know that Democritus wrote several books about the mind and senses. Some of the known book titles were “On the Mind”, “On the Senses”, “On Flavours”, “On Colours”, and “On Logic” (book titles back then were still simple). Unfortunately, none of Alcmaeon’s and Democritus’ books survived the passage of time, all we know are the book titles and the references from other philosophers. Who knows where we would have been today, if only we would have managed to preserve the memory of these ancient giants.&lt;/p&gt;

&lt;p&gt;The Ancient Greeks answered the question of ‘‘where’’ the mind takes place. Today’s scientists on the other hand are trying to answer ‘‘how’’ the mind does what it does. How do we learn? Where and how are memories stored? How does consciousness form? These are quite abstract questions, which because of the way they are asked, are difficult to answer. I strongly believe that to answer these questions, we will have to question every neurobiological structure and biochemical process we see. Why is there a very long dendrite that emerges from the cell body of pyramidal cells? Why are so many excitatory neurons covered with dendritic spines? Why are most interneurons spineless? What’s the purpose of the back-propagating signal that occurs inside a neuron? Why do excitatory and inhibitory neurons look so different? What is the purpose of the enigmatic spine apparatus and why do axons also have such a similar organelle?&lt;/p&gt;

&lt;p&gt;I believe that these are the kind of questions that will allow us to reverse engineer the circuits that give rise to the algorithms of our minds. As Albert Szent-Györgyi once said: “If structure does not tell us anything about function, it only means we have not looked at it correctly.” Patterns that repeat over and over again in nature tend to have an important role. In biology after all, form follows function. Instead of focusing on big philosophical questions, I argue that we can deduce and understand the algorithmic parts of the brain by questioning the patterns in the morphology and biochemical processes of neurons. Using a reductionist approach alone however, will not be enough to decipher the mind. a bottom-up approach might generate a lot of facts, but not necessarily new ideas that will lead to discoveries. For that, a holistic mindset is necessary. I am not advocating one mode of thinking over another, on the contrary. We will need to be willing to constantly switch between modes of thinking, to truly tackle complex problems like the mind. As you can quickly notice, one needs to know some basic neurobiology, to understand the questions this book tries to answer. That’s why in the first part of the book, we are going to learn some basic neuroscience.&lt;/p&gt;

&lt;p&gt;The main focus of this book will be about how biological memories form and are stored. The whole first part of the book in fact will deal with that topic. I will describe a concrete algorithm for how memories are formed and stored in neural circuits, and how a phenomena called ‘‘neuronal specialisation’’ can emerge by following very simple rules. My proposed model aims to extend Hebb’s rule, with the necessary temporal attributes for an asynchronous system to function. We will see how neural competition is key for Hebb’s neural assemblies to form. Once we know how this model works, I will also explain what the purpose of some neural structures might be from the context of the mind.&lt;/p&gt;

&lt;p&gt;My interest is not to write a philosophical, vaguely defined proposal for how memories are formed and stored. Instead I will use a more pragmatic approach and explain a concrete, simple to understand process for it. Even though we do not know all the biochemical puzzle pieces for this problem yet, we will see how by looking at many experiments, we can already come up with a simple unsupervised learning algorithm for neural circuits. I feel it is important to emphasise that while designing the model, I tried to be as strict as possible on its biological plausibility, putting neurobiology first and machine learning second. There will be no no unnecessary abstractions that have little to do with actual biology. This book is after all meant to help us get closer to deciphering the brain.&lt;/p&gt;

&lt;p&gt;Instead of immediately trying to answer the big questions, we are going to ask well defined biological questions, like the ones mentioned earlier. We are then going to use the proposed answers to those questions as puzzle pieces for bigger questions, like ‘‘How are memories stored?’’. Often times knowing how to ask the right questions is even more important than the question itself. We will not only examine the proposed model, but the biological structures from which it emerges as well. We will see how many of the questions in neurobiology, including the ones mentioned earlier in this introduction, can be indirectly explained and understood through this model. Moreover, the model that I will propose will also reveal the existence of a mysterious code inside neurons. We will discuss the implications of such a code, as well as the evidence supporting the existence of such a code, in great detail.&lt;/p&gt;

&lt;p&gt;As you have probably seen in the contents page, the first part of the book is called “Biology”, whereas the second part “Technology”. While the first part describes the neurobiological structures and the emergent phenomena of neural circuits, the second part of the book focuses on the potential real-world applications of the proposed model. In the second part of the book, I will discuss about artificial neural networks, as well as the emergent field of neuromorphic computing. We will see what the basic idea of machine learning is and how the concepts of machine learning can be quite a useful mental model to reverse engineer the algorithms of biological systems. I will also explain how todays neural networks work, and why I believe that it is so important to figure out alternatives to the famous backpropagation algorithm. At the end of the second part, we will explore some of the existing neural network architectures and I will propose a new kind of architecture based on the proposed model from the first part of the book.&lt;/p&gt;

&lt;p&gt;Writing this book turned out to be tricky. On the one hand, the book has to be technical enough so that scientists and researchers do not get annoyed, yet it has to be simple and clear enough, so that people from different backgrounds can nonetheless understand everything. Because of this, I decided that any term or concept used after the introduction, be it neuroscience related or machine learning related, has to be explained in advance. My interest is after all to reach as many people as possible. If we want to inspire someone to adopt a new idea after all, the things we write have to be readable. I also hope that this book will blur a bit the lines between fields. The brain is a complex system, meaning that it consists of many components that interact with one another. Complex systems are inherently hard to model. Knowing the components of the system is only one part of the solution, we also have to figure out how those parts interact. To achieve this, I argue that we have to use an interdisciplinary approach. When studying things, we should not only strive for depth of knowledge, but range as well. Reading papers and books from other fields, can foster lateral thinking and help us come up with completely new ideas. Even if the idea that I will present will turn out not to reflect reality, I hope that the facts presented in this book, and the questions asked will help others decipher the algorithms of the mind.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/Introduction</link>
        <guid isPermaLink="true">http://localhost:4000/Introduction</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>Using binary vector clocks to replace Ethereum transaction nonces</title>
        <description>&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;ol id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#contents&quot; id=&quot;markdown-toc-contents&quot;&gt;Contents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-order-of-ethereum-transactions&quot; id=&quot;markdown-toc-the-order-of-ethereum-transactions&quot;&gt;The order of Ethereum Transactions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#partial-orders-and-join-semilattices&quot; id=&quot;markdown-toc-partial-orders-and-join-semilattices&quot;&gt;Partial Orders and Join-Semilattices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-binary-vector-clock&quot; id=&quot;markdown-toc-the-binary-vector-clock&quot;&gt;The Binary Vector Clock&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-inevitable-total-order-during-epoch-jumps&quot; id=&quot;markdown-toc-the-inevitable-total-order-during-epoch-jumps&quot;&gt;The inevitable total order during epoch jumps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;abstract&quot;&gt;abstract&lt;/h1&gt;
&lt;p&gt;In this blog post, I’ll present the idea of a Binary Vector Clock, a simple, yet space-efficient algorithm for generating a partial order of transactions in account-based blockchain systems. The Binary Vector Clock solves the problem of order dependency in systems such as Ethereum, caused by the total order of transactions that come from the same address holder. What that exactly means will become clear in a bit. The proposed algorithm has the same security as using regular transaction nonces, requires very little overhead, and can potentially result in a significant increase in throughput for systems like Ethereum. This paper was originally submitted on the 15th of April, 2020 on &lt;a href=&quot;https://arxiv.org/abs/2004.07087&quot;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;There are generally two kinds of transaction models in blockchains: The UTXO model, and the account based model. The UTXO model was the first transaction model to be proposed and has many intriguing properties. In this paper however, we are going to focus on the account based model, more exactly on the one implemented by Ethereum. In the account based model, instead of having coins as unspent outputs like in the UTXO model, every participating node has an account, or a balance. When a transaction is created, the transaction’s value is simply reduced from the owners account, and added to someone else’s account. To understand the problem that the Binary Vector Clock tries to solve, let’s first look at the structure of an Ethereum transaction and how the &lt;em&gt;order&lt;/em&gt; of transactions is determined.&lt;/p&gt;

&lt;h1 id=&quot;the-order-of-ethereum-transactions&quot;&gt;The order of Ethereum Transactions&lt;/h1&gt;
&lt;p&gt;A transaction in Ethereum is essentially a message that gets signed by an account holder, also known as an externally owned account. Once a transaction gets created, it is broadcast to other nodes in the system, and eventually recorded by the Ethereum blockchain. The structure of an Ethereum transaction consist of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;em&gt;value&lt;/em&gt; (the amount of ether we want to transfer).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;recipient&lt;/em&gt; (the address of the account to whom we want to send the transaction to).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;gas price&lt;/em&gt; (much like a transaction fee. The gas price shows how much of a fee the originator of the transaction is willing to pay).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;gas limit&lt;/em&gt; (the maximum fee that the originator is willing to pay).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;v,r,s&lt;/em&gt; (the three ECDSA digital signature components to prove that the originator truly formed the transaction).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;data&lt;/em&gt; field (an optional field that can contain code, for when an account interacts with smart contracts).&lt;/li&gt;
  &lt;li&gt;And the &lt;em&gt;nonce&lt;/em&gt; (an account specific counter. Whenever a transaction from the address holder gets confirmed, the counter increments).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The nonce field is the field that is of particularly interest to us. The transaction nonce, not to be confused with the block nonce used for Proof of Work, is a scalar value that serves as a counter. The nonce shows the number of confirmed transactions that originated from the account. Having such a counter for each transaction has an interesting effect: It protects the user from transaction duplication. Let’s see what would happen if transactions had no nonce, to better understand why having such a counter is so important: Let’s say Alice sent Bob a completely valid transaction containing three ether. The signature turned out to be truly Alice’s, and the transaction got recorded on the blockchain. Bob however turns out to have a bad moral compass and wants more money. Without a transaction nonce, there is nothing to stop Bob from “replaying” Alice’s transaction, and claim again three ether. Bob could in fact repeat transmitting Alice’s old transaction to the network, until he gets all of Alice’s ether. Every time the transaction would be replayed, nodes in the system would think that it is a new transaction. In reality however, this is not what happens. By having a counter attached to the transaction, every transaction becomes unique. If let’s say Alice’s transaction has a nonce of 42, Bob will not be able to replay that transaction, as any new transaction coming from Alice would have to have a nonce greater than 42.&lt;/p&gt;

&lt;p&gt;There is however also another important reason to have a nonce in an account-based transaction: We want to be able to determine the &lt;em&gt;order&lt;/em&gt; of transactions. Let’s assume this time that Alice is sending two transactions, but the second transaction is dependent on the first one, i.e. running the second transaction before the first one is invalid (for whatever reason). In a centralized system this is no problem, one would simply confirm the first transaction first, and than continue with the second transaction. In a decentralized system however, nodes in the network might receive the second transaction before the first one. We cannot know in advance in which order nodes will perceive events. Without a counter, there would be no way for nodes in the network to tell which transaction comes first. If on the other hand the first transaction has a counter of 42 and the other transaction has the next counter (43), the order can be determined. If a node in the network thus receives the second transaction before the first one, it knows that it should ignore the second transaction, until the first transaction gets confirmed.&lt;/p&gt;

&lt;p&gt;This is a great feature, but it also has its shortcomings. If Alice were to send several transactions one after another, and one of the transactions does not get included in any block for some reason, e.g. the transaction turns out to be invalid, then none of the subsequent transactions get processed. Only after providing a transaction with the missing nonce, do all the other transactions get processed.
This is no problem if every transaction depends on the previous one, but in most real-world applications that would not be the case. Many nodes have to create dozens of transactions in a short period of time, imposing an order dependency thus can result in transactions having to stay in mempools, even if they could have been processed sooner. The total order of transactions represents at the same time a great feature, and a serious scaling problem for account-based transaction models. In the following sections, I will present how we can overcome the problem of total order when processing transactions.&lt;/p&gt;

&lt;h1 id=&quot;partial-orders-and-join-semilattices&quot;&gt;Partial Orders and Join-Semilattices&lt;/h1&gt;
&lt;p&gt;Before jumping straight to how the Binary Vector Clock works, it is necessary to have a good grasp of what a partial order is. All of us intuitively understand the idea of “total orders” - One is smaller than two, five is greater than four, etc. In order theory, a set is said to have a total order, if for any element $a$ and $b$, a comparison is possible, i.e. either $a \leq b$ or $a \geq b$. For example: Every transaction nonce for an address, is comparable to any other transaction nonce for that address. We thus can easily know which transaction happened-before another transaction, thanks to the total order of transactions. But what if it does not matter in which order some of our transactions get confirmed? If eight out of ten transactions generated from an address holder could in fact be confirmed in any desired order, it would be quite wasteful not to do so. This is however what happens in today’s totally ordered account based transaction model.&lt;/p&gt;

&lt;p&gt;It would thus be of enormous interest if we could somehow “capture” the transaction independence for address holders. This is where partial orders become useful. A partially ordered set, is a set in which only certain pairs of elements are comparable, i.e. one element precedes the other in the ordering, but not every pair of elements is necessarily comparable.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot; height=&quot;300&quot; src=&quot;/assets/images/posts/2020/semilattice.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As an example to better understand what a partial order actually is, let’s look at the join-semilattice in figure 1. The diagram shows a set $S$ with eight vectors. We say that an element in $S$ ‘‘happened-before’’ another element, if and only if every value of vector $a$ is less than or equal to every corresponding value in vector $b$. For example: We can conclude that vector $(1,0,0)$ happened before vector $(1,1,0)$, because none of the values in vector $(1,0,0)$ are greater than in vector $(1,1,0)$ - We say that $(1,0,0)$ happened-before $(1,1,0)$. If on the other hand we try to compare vector $(1,1,0)$ and $(1,0,1)$, one can see that both vectors have values larger than the other vector at some indices. We say that this pair is &lt;em&gt;not comparable&lt;/em&gt;. One cannot determine which element occurred before the other one. Algorithms used in distributed systems, such as vector clocks, take advantage of partial orders. In the context of the distributed systems, having incomparable vectors, or “clocks”, usually means that the events occurred concurrently, and thus have no information of one another. In the case of the Binary Vector Clock on the other hand, incomparableness between two transactions does not indicate concurrency, it indicates that they occur &lt;em&gt;independently&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;the-binary-vector-clock&quot;&gt;The Binary Vector Clock&lt;/h1&gt;
&lt;p&gt;Let’s imagine that instead of a nonce (i.e. counter) for a transaction, we have a counter &lt;em&gt;and&lt;/em&gt; a very small bit array (for the sake of a better explanation, let’s stick to three bits, like the vectors in figure 1. Alice’s Binary Vector Clock is initially set to $(0, [0,0,0])$ (where the first element represents the counter and the second element the bit array). For simplicity, I will refer to the Binary Vector Clock from now on as a “timestamp”. Now let’s say Alice wants to send three transactions one after another. Alice however knows that her second transaction is dependent on her first transaction, but her third transaction has no logical dependency to the two first transactions. Having this information, Alice can do something clever: Instead of incrementing her counter for each transaction, she increments one of the bits in her bit array. Let’s say the first transaction has the timestamp $(0,[0,0,1])$, the second transaction has the timestamp  $(0,[0,1,1])$, and the third timestamp is $(0,[1,0,0])$. All three transactions were send one after another to the network. Any validator receiving the transactions can independently know in what order the transactions need to be confirmed (or if any order exists at all). Validators first look at the counter, the counter tells a validator if the transaction is in the right “epoch” (more on that in a bit). If the counter is equal to the previously confirmed transaction from that address, the bit array is checked. As the bit array of the first and third transaction are not comparable (no order can be determined), even if the first transaction turns out to be invalid for some reason, the third transaction can still be processed by the validators. This is because both timestamps are indicating “independentness”, there is no “happened-before” relationship between them. The second and the first transaction on the other hand do have a “happened-before” relationship. When looking at the bit array of the second transaction, we can conclude that it must have happened after the first transaction. If a validator thus would receive the third transaction and the second transaction, but not the first transaction for some reason, it would know that the third transaction can be processed, but the second transaction not, as it depends on a prior transaction (the first transaction). If a transaction gets confirmed, the address’ Binary Vector Clock gets simply added with the newly confirmed timestamp. Taking again the three transactions from the previous scenario as an example, if Alice’s initial timestamp was $(0,[0,0,0])$, and her first and third transactions get confirmed, her new timestamp would be $(0,[1,0,1])$. Once all the bits in the bit array are turned to one, we can increment the timestamp’s counter, and set the bit array to zero again. We call this shift an “epoch”.&lt;/p&gt;

&lt;p&gt;Up to this point, some of the readers might have already thought something in the lines of: But what if Alice has only one ether, and she creates three independent transactions, each spending one ether? It is important to remember that this is an issue only if transactions would be processed concurrently, which is not the case with the Binary Vector Clock technique. In cases like the one mentioned above, transactions would be treated the same way today’s transactions get treated, if they were to have the same nonce. Today, with the nonce approach, if transactions have the same nonce, one of the transactions would get confirmed (depending on the block creator) and the rest of the transactions would become invalid. In the case of the Binary Clock, one of Alice’s transactions (depending on the block creator) would get confirmed, while the rest of the transactions would simply be considered invalid, regardless of their order independency.&lt;/p&gt;

&lt;h1 id=&quot;the-inevitable-total-order-during-epoch-jumps&quot;&gt;The inevitable total order during epoch jumps&lt;/h1&gt;
&lt;p&gt;It is important to note that there is nonetheless an inevitable transaction processing dependency when shifting from one epoch to the next. Transactions from one epoch can only be processed independently, after the transactions of the previous epoch were already processed. In other words if Alice were to send three other transactions one after the other, where the first transaction would have a timestamp of $(0,[1,1,1])$, second transaction $(1,[1,0,0])$, and third transaction $(1,[0,0,1])$, even if all three transactions are completely independent from one another, the second and third transactions will not be able to get processed without the first one being confirmed first. This is because these transactions occurred during an epoch “jump”, i.e. the Binary Vector Clock gets incremented, and the bit array becomes set to zero. The transactions in the new epoch cannot know if they are comparable or not with the transaction from the previous epoch, forcing a momentary total order. I argue however that the space-wise inexpensive nature of the Binary Vector Clock, and its property to handle partial orders, makes it an attractive technique for the account-based transaction model, even in the case of momentary order dependencies between epoch jumps.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In This paper I introduced the Binary Vector Clock, a memory-wise inexpensive partially ordered counter for account-based transactions, that solves the issue of order dependency when processing transactions. Note that the Binary Vector Clock does not suggest the concurrent processing of transactions in Ethereum. Doing so would in fact introduce many possible attack vectors to the system. It only specifies which transactions can be processed independently, and which ones depend on a prior transaction confirmation. If for example an address generates $N$ transactions one after another, and the first transaction fails, the subsequent transactions are still able to get processed and confirmed by the blockchain. This is not the case in today’s approach with transaction nonces. In today’s approach, if the first transaction fails for some reason, all of the other transactions would need to be ignored until the gap in the nonce becomes filled. The Binary Vector Clock overcomes the issue by introducing a partial order between transactions of the same address holder. Using the Binary Vector Clock as a substitution for the transaction nonce gives more freedom to the user in determining transaction orders. The Binary Vector Clock allows the user to specify if a transaction can be processed  independently from other transactions, or if it should be queued until a certain transaction gets confirmed. I argue that this ability has important implications for blockchain systems. Considering that transactions in blockchain systems most likely follow a pareto distribution (the majority of transactions are generated by very few nodes), introducing an inexpensive technique that allows for independent processing of transactions, could potentially increase the scaling capability of Ethereum and other account-based blockchains significantly.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/The-Binary-Vector-Clock</link>
        <guid isPermaLink="true">http://localhost:4000/The-Binary-Vector-Clock</guid>
        
        <category>Paper</category>
        
        <category>Vector Clock</category>
        
        <category>Ethereum</category>
        
        
      </item>
    
  </channel>
</rss>
