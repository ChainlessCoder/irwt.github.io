<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lum Ramabaja</title>
    <description>Welcome to my blog</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 27 Sep 2020 22:48:19 +0200</pubDate>
    <lastBuildDate>Sun, 27 Sep 2020 22:48:19 +0200</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>The Ghost In The Neuron</title>
        <description>&lt;p&gt;This post is part of an ongoing online book. To access the other parts, please refer to the contents page of the &lt;a href=&quot;https://lums.blog/The-Competitive-Neuron&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;In this chapter, we will focus on two topics. In the first part, I will lay down the necessary puzzle pieces required to understand how memory is formed and stored in neural circuits. We will take a closer look at the morphology of various neurons, as well as question the purpose of certain neural structures. In the second part of the chapter, we will investigate the enigmatic process known as “coincidence detection”, i.e. the ability of neurons to specialize to certain inputs. Before diving straight to the interesting bits however, it is important to establish a common language. It wouldn’t make much sense to try and figure out how an engine works, without first knowing its parts. The first step towards reverse engineering the algorithms that give rise to engrams, is to understand how the underlying hardware works.&lt;/p&gt;

&lt;p&gt;In schools, we are usually taught that a neuron takes in signals sent from other neurons. It then integrates the received signals, and if the integrated inputs exceed a certain threshold, the neuron fires a signal to other neurons. This is also known as the integrate and fire model. If you still remember this fact from a biology class, I want you to forget it. One of the most difficult things for people who have finished an education, is to unlearn the idea that “the way we model the world, represents the way the world is”. Only after realizing that &lt;em&gt;all&lt;/em&gt; human models are wrong, can we overcome our biases to come up with completely new ideas. We are often taught to imagine neurons as thin branching wires that simply integrate incoming signals and fire. I argue that treating neurons as such a simplistic systems can often do more harm than good. A single neuron is in fact unbelievably complex. My goal in this chapter, besides laying down the necessary puzzle pieces for later chapters, is to convince you to look at neurons with a different light. Instead of imagining neurons as simple logic gates, you will (hopefully) look at them as unbelievably powerful computers.&lt;/p&gt;

&lt;p&gt;Neurons are not only complex, but diverse as well. We usually talk about neurons, as if there was only one type of neuron, but that is far from correct. There is, in fact, a whole jungle of different neurons in your brain, each possessing different abilities and morphology. Some neurons, like the &lt;em&gt;interneurons&lt;/em&gt; shut down, or &lt;em&gt;inhibit&lt;/em&gt; other neurons when sending signals to them. Some other neurons might have the opposite effect and turn on, or &lt;em&gt;excite&lt;/em&gt; other neurons. Even though there are many different kinds of neurons, we nonetheless refer to them all with the shared name “neurons”, due to their similar electrical properties. They all are able to accept and send signals to other neurons. Every neuron has a cell body, or &lt;em&gt;soma&lt;/em&gt;, as any other cell. The soma houses the genome of the neuron, most of the gene expression happens at this part of the cell. It also houses a variety of organelles, small biological machines floating in the fluid of the soma. From the soma, a long fiber like structure emerges, called the &lt;em&gt;axon&lt;/em&gt;. The axon is where a neuron initiates action potentials, aka spikes, aka signals (more exactly at a part of the axon called the axon hillock). Once we go along the axon, it further splits into many branches, at the end of which there are &lt;em&gt;axon terminals&lt;/em&gt;. Axon terminals are the parts of the axon that actually connect to other neurons. The axon thus serves as the “output pole” for the neuron, this means that it transmits signals to other neurons by firing spikes through the axon, to the axon terminals, and that way to other neurons.&lt;/p&gt;

&lt;p&gt;Beside the axon, there are also many other branching fibers that come out of the soma, also known as &lt;em&gt;dendrites&lt;/em&gt;. If we imagine axons as the output pole of a neuron, then the dendrites are its “input pole”, in other words they receive the signals sent from other neurons. So the dendrites accept signals from other neurons, whereas the axon transmits signals to other neurons, and both are connected to the cell body, aka the soma. As in any living thing, all the mentioned parts of the neuron are covered by a cell membrane, also known as a lipid bilayer. Because of it, the inside of the neuron has a different resting potential, than the outside of the neuron. The electrochemical properties, and mechanisms of how the membrane achieves such a contrast is well beyond this book. For now just take into consideration that a neuron’s membrane is covered with proteins that serve as gates to the outside world. These proteins are often referred in the literature as pumps, or channels, depending on their properties. The inside of a neuron, as well as the fluid surrounding the neurons, contains various ions, such as Cl-, Na+, K+, Ca2+. The proteins residing inside the lipid bilayer act as gates, allowing some of the ions to flow inside the neuron, and pump other ions out of the neuron, depending on conditions. This causes the inside of the neuron to have a different charge from the outside fluid.&lt;/p&gt;

&lt;p&gt;When a neuron’s dendrites are stimulated by a neuron that is emitting spikes, aka by a &lt;em&gt;pre-synaptic&lt;/em&gt; neuron, it causes some of the channels from the neuron that is receiving the spikes, aka the &lt;em&gt;post-synaptic&lt;/em&gt; neuron , to quickly open, allowing some ions to rapidly flow into the cell, further increasing the positive charge of the neuron. This simple process can result in a chain reaction of events, creating a wave of ions that travel rapidly along the axon, and trough its axon terminals along its way. This wave is also known as an action potential, or a spike. In other words, action potentials are the signals that get transmitted from one neuron, to another. The axon terminals of a neuron are very close (but do not touch) to the dendrites or somas of other neurons. The small region where an axon terminal “connects” to another neuron is called a &lt;em&gt;synapse&lt;/em&gt;, and the narrow gap between the two neurons is also known as the &lt;em&gt;synapatic cleft&lt;/em&gt;. Imagine it as a tiny space between the axon terminal of one neuron, and the dendrite of another neuron. The previously mentioned action potentials, after reaching the axon terminal cause the axon terminals to release chemicals (also known as &lt;em&gt;neurotransmitters&lt;/em&gt;) to the synaptic cleft. The released neurotransmitters can result in the opening of ion channels at the post-synaptic neuron. This procedure is also known as &lt;em&gt;chemical transmission&lt;/em&gt;, and it’s the main mechanism for how neurons communicate with one another.&lt;/p&gt;

&lt;p&gt;As a recap, neurons get signals from other neurons via their dendrites, long fibre looking structures that come out of the soma (the cell body). The signals flow from the dendrites to the soma, where they are somehow integrated, and then a signal gets fired through the axon, which reaches the other neurons via the axon terminals. This is a very simplistic depiction of how information flows from one neuron to another. So in a nutshell, the terms you have to remember and understand before being able to continue are: soma, axon, dendrites, synapse, synaptic cleft, neurotransmitters, pre-synaptic neuron, post-synaptic neuron, and the basic idea of excitatory neurons and inhibitory neurons. Now that the terminology is cleared, we can go on with the actual chapter.&lt;/p&gt;

&lt;p&gt;As mentioned before, there are many different kinds of neurons in the brain. One of the most notable ones is the pyramidal neuron, which was first discovered and extensively studied by Santiago Ramón y Cajal back in the 19th century. It is the most common type of excitatory neuron in the mammalian cortical structures, as well as one among the largest neurons in the brain. It is also believed that they play an important role in advanced cognitive functions. One can find pyramidal cells in different brain parts, such as the cerebral cortex, hippocampus, and amygdala, each of which plays a key role in memory (as well as many other functions). Every single mammal has pyramidal neurons, but it is not only mammals, birds, fish, and reptiles also poses them. The fact that we can find pyramidal neurons across many distant species is quite intriguing.&lt;/p&gt;

&lt;p&gt;Besides its wide occurrence in nature, the pyramidal neuron also has some interesting morphological characteristics. Both its dendrites and axon seem to be branched extensively, which allows it to communicate with thousands of other distant neurons. Like other kinds of neurons, the pyramidal neuron has many branching dendrites that come out of the soma, also known as &lt;em&gt;basal dendrites&lt;/em&gt;. Besides the basal dendrites however, there is also another, single, long, dendrite that emerges from its soma. This long dendrite is also known as the &lt;em&gt;apical dendrite&lt;/em&gt;, and it can extend for several hundred microns before branching into many smaller dendrites. The cluster of branches at the end of an apical dendrite is also known as the &lt;em&gt;apical tuft&lt;/em&gt;. Synaptic information flows from the apical tuft, through the body of the apical dendrite, all the way to the soma. The soma thus receives the signals from several places, its apical dendrite and its basal dendrites, and then “decides” if it should fire or not. We &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/8743416/&quot;&gt;know&lt;/a&gt; for example that the shorter basal dendrites receive their input from local pyramidal cells and interneurons, whereas the long apical dendrite receives its inputs from more distant regions.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;650&quot; height=&quot;650&quot; src=&quot;/assets/images/posts/2020/Pyr_fig2.jpg&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Why would pyramidal neurons posses such a long dendrite? Some researchers believe that it’s more energy efficient for a neuron to have one long dendrite with lots of branches at the end, than many long dendrites. This certainly makes sense, but could there also be another reason? There are for example other types of neurons, like the stellate cells, that do not have an apical dendrite at all. But we also know that most stellate cells are inhibitory in nature, whereas the Pyramidal neuron with its apical dendrite is excitatory. Why would some neurons posses apical dendrites, whereas some others not? Understanding the role of the apical dendrite will be our first puzzle piece. Once we realize that we’re dealing with an asynchronous system, the existence for an apical dendrite starts to make more sense. For now just keep in mind that this interesting feature exists, we will get back to it in the next chapter again.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;400&quot; height=&quot;400&quot; src=&quot;/assets/images/posts/2020/cajal_spines2.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Another fascinating and mind boggling structure of the pyramidal neuron, is the so called dendritic spine, which was also discovered and thoroughly studied by Santiago Ramón y Cajal. Dendritic spines are tiny protrusions that completely cover the apical dendrites and basal dendrites of pyramidal neurons. When looked under a microscope, we can see the spines cover the whole dendrites, much like leaves on a tree. Cajal named them ‘‘dendritic spines’’, because when he studied them, they looked like spines on a rose stem (even though now we know that dendritic spines are more round and not really spiny in shape). Today we know that dendriditc spines are where most synapses form. But even back then, Cajal speculated that spines were the part of the neuron that received axonal inputs from other neurons. If you remember from earlier on, that is in fact the definition of a synapse. He also had another interesting &lt;a href=&quot;https://mitpress.mit.edu/books/dendritic-spines&quot;&gt;thought&lt;/a&gt; about spines: Cajal believed that physical changes in dendritic spines could somehow be associated with learning. Today, the overwhelming evidence shows that this is indeed the case.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot; height=&quot;300&quot; src=&quot;/assets/images/posts/2020/cajal_spines1.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Dendritic spines are in fact the&lt;/p&gt;

&lt;p&gt;There can be various stages, or classes of dendritic spines, the exact category borders of which are a bit blurry. In general we classify spines into thin, stubby, and mushroom spines. Thin spines have a small head and a long neck that connects to the parental dendrite. Stubby spines have no neck at all and are more expressed in infants. Whereas mushroom spines have a wide neck with a large head and are more expressed in adult brains \citet{Yuste2010DendriticSpines}.&lt;/p&gt;

&lt;p&gt;They were first discovered and named by Santiago Ram'on y Cajal when observing the surface of Purkinje cells, which are another type of neurons that are studded with dendritic spines.&lt;/p&gt;

&lt;p&gt;It is believed that dendritic spines somehow serve as information storage components. Indeed, there are a lot of puzzling facts about the dendritic spine once you study it. First, it is now clear that spines receive most of the excitatory input in neurons \citet{Yuste2010DendriticSpines}. Remember when I described synapses in the previous chapter? Practically all the excitatory synapses a neuron has, form on dendritic spines \citet{Yuste2010DendriticSpines}. In other words excitatory axons prefer for some reason to terminate particularly on spines, they almost never connect directly to dendritic shafts (i.e. the body of the dendrite). Why is this the case? Does it really matter that much where a signal enters a dendrite?&lt;/p&gt;

&lt;p&gt;To make things even stranger, we can see the opposite behavior in inhibitory axons. Inhibitory axons almost always connect directly to dendritic shafts, rather than on spines \citet{Yuste2010DendriticSpines}. ‘‘This can’t be a coinsidence’’ you might think, and I’d argue that you’re right. When we start to zoom a bit out from the spine and focus on the neuron as a whole, we can observe another strange fact: Excitatory neurons like the Pyramidal neuron all have dendritic spines. Whereas inhibitory neurons, like the internerons which we will talk about in a bit, are most of the time spineless \citet{Yuste2010DendriticSpines}.&lt;/p&gt;

&lt;p&gt;Why would axon terminals prefer different sites depending on their signaling nature? Why are so many neurons covered in spines, while some others barely have them? Could there be an epiphenomenon at play here? The presence of dendritic spines, or the lack of them, is our next puzzle piece for the Koha model. For now, let’s jsut keep these facts in mind.&lt;/p&gt;

&lt;p&gt;A neuron can be studded with hundreds of thousands of dendritic spines. As many scientists before me, I also argue that dendritic spines are in fact computational units that serve as memory storage. Even Cajal who discovered dendritic spines a century ago, argued that physical changes in spines could be associated with learning. Many scientists believe that spines serve as storage sites for synaptic strength (a term we will explore in later chapters). I believe however that spines serve a much bigger role than just that.&lt;/p&gt;

&lt;p&gt;Nowadays we know from imaging experiments that dendritic spines are indeed biochemical compartments, more specifically calcium compartments \citet{Yuste1995DendriticIntegration.}. The narrow neck of the spine can create an isolated compartment in which the biochemical signals of the spine head do not spread along the parent dendrite \citet{Volfovsky1999GeometryExperiments}. To some readers this might sound like a boring biology fact, but the implications of it are enormous. Many scientists think that because of the compartmentalized nature of spines, they could potentially act as functional units inside dendrites. In layman terms, spines could be responsible for determining how incoming signals are integrated.&lt;/p&gt;

&lt;p&gt;There are now many experiments that support this claim. We now know for example that the most significant filtering of local potentials does not happen along the dendrite, but instead at the spine neck \citet{Araya2006ThePotentials}. The spine neck is somehow able to modulate the amplitude of incoming signals. Further studies have also shown that spines, additionally to filtering out local potentials, are also able to amplify them \citet{Araya2007SodiumPotentials.}. How does a spine know what kind of modulation to apply to local potentials?&lt;/p&gt;

&lt;p&gt;Interestingly, spines in pyramidal neurons are full with some receptors known as NMDA receptors. These receptors are quite slow compared to other excitatory receptors. Because of this (and some other studies), some scientist like Rafael Yuste for example, believe that one thing spines do, is slow down the synaptic inputs a neuron receives. But why would a neuron want to slow down its inputs? One would assume that evolution would have optimized neurons for speed, having faster reaction times is after all an important feature. That is not what we see however. But instead of thinking things like ‘‘What a poorly designed machinary’’, we should think ‘‘hmm, what could be so important, that slowing down signals could become advantageous?’’. The pyramidal neuron after all has passed the test of time.&lt;/p&gt;

&lt;p&gt;With all the strange facts we collected so far, I would like to propose something even stranger. I propose the existence of a Spinal code. We will discuss about potential candidates for the physical manifestation of the code in the next chapter. For now, just imagine that each dendritic spine has a fingerprint which allows it to detect a specific temporal pattern in its input stream. As looney as the proposal might sound, things will make sense in the next chapter. The strange apical dendrite of the pyramidal neuron, the presence or absence of dendritic spines in excitatory and inhibitory neurons, the strange preference of axon terminals to connect to dendritic sites depending on their signaling nature, the ability of spines to dynamically modulate local potentials, all this and more will start to make sense by introducing the existence of a spinal code. However, as Richard Feynman once said:
\begin{displayquote}
‘‘It doesn’t matter how beautiful your theory is, it doesn’t matter how smart you are. If it doesn’t agree with experiment, it’s wrong.’’
\end{displayquote}
As I claimed earlier, I tried to be as strict as possible to the model’s biological plausibility. The existence of a Spinal code, even though purely hypothetical, will start to sound plausible,  after we explore the interesting facts of the next chapter.&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Jun 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/the-ghost-in-the-neuron</link>
        <guid isPermaLink="true">http://localhost:4000/the-ghost-in-the-neuron</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>Using Medoka encoding to compress sparse bitmaps</title>
        <description>&lt;p&gt;In this short article, I am going introduce the idea of Medoka encoding, a simple lossless compression technique which I invented a couple of years ago. In contrast to the &lt;a href=&quot;https://en.wikipedia.org/wiki/LZ77_and_LZ78&quot;&gt;LZ family&lt;/a&gt; of lossless compression algorithms, Medoka encoding can only be applied to bit arrays, aka bitmaps. The technique is unbelievably simple to code, it is highly parallelizable, very quick in both encoding and decoding, and it usually performed better in my experiments than LZ techniques (of course, more rigorous experiments have to be done for that claim).&lt;/p&gt;

&lt;p&gt;Before explaining how the encoding scheme works, let’s look at some simple use cases for bitmap compression:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bitmap_index#Compression&quot;&gt;Bitmap index&lt;/a&gt; compression. Bitmap indices are just bit arrays used in databases to answer queries by performing bitwise logical operations. In some cases however, our data columns can be quite sparse. In other words our bitmap would have a lot of zeros, and just a few ones. When dealing with such cases, we usually try to compress the bitmap, so as to not waste unnecessarily space.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bloom_filter&quot;&gt;Bloom Filters&lt;/a&gt;. Bloom filters are probabilistic data structures used to test whether an element is a member of a set. They are probabilistic, because false positives can occur while checking for the presence of elements. Bloom filters are also just bit arrays. The ratio of ones and zeros in the bloom filter depends on the chosen parameters of the bloom filter, as well as on the number of elements inserted to the bloom filter. If the difference between zeros and ones is large, it’s better to compress the bloom filter, before sending it to another node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s important to note that in both mentioned cases, if we were to use a compression technique such as LZ77, we would have first to decode the data, before being able to query it. Medoka encoding does not have this problem, one  can directly participate in bitwise operations without decompressing the data.&lt;/p&gt;

&lt;p&gt;Now let’s look at how Medoka encoding works, by following simple Python code:&lt;/p&gt;

&lt;p&gt;First let’s create a random sparse list with ones and zeros:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
a = (np.random.rand(100) &amp;lt; 0.1).astype(int).tolist()
print(a)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output will look something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To perform Medoka encoding, we first need to transform the bit array. For the sake of it, let’s call the transformation “sum_transform”:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def sum_transform(a):
  mapp = {1:1, 0:-1}
  s = []
  n = 1
  prev = a[0]
  for i in a[1:]:
      if i == prev:
          n += 1
      else:
          s.append(n * mapp[prev])
          n = 1
          prev = i
  s.append(n * mapp[prev])
  return s

b = sum_transform(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[-7, 2, -3, 1, -3, 1, -19, 1, -7, 2, -2, 1, -18, 1, -20, 1, -11]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we can see, “sum_transform” takes as input the bit list. To better understand what we’re actually doing at this step, look at both the bit list output, as well as the sum_transform output. The function sums up all the zeros that are next to each other and assigns a negative sign to that integer. It does the same thing to the ones, and assigns a positive sign to the integer.&lt;/p&gt;

&lt;p&gt;Now comes the final step of the Medoka encoding algorithm:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def medoka_encoding(a, symbol):
    b = [a[0]]
    for elm in a[1:-1]:
        if elm != symbol:
            b.append(elm)
    b.append(a[-1])
    return b

c = medoka_encoding(b, 1)
print(c)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[-7, 2, -3, -3, -19, -7, 2, -2, -18, -20, -11]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the final step of the Medoka encoding algorithm, we simply specify the most frequent symbol from out “sum_transform step”, which was the number one. We then remove all ones from sum_transform list. What we get at the end is our final array, that is much more compressable now. We can then compress the array either by using &lt;a href=&quot;https://en.wikipedia.org/wiki/Huffman_coding&quot;&gt;Huffman coding&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential-Golomb_coding&quot;&gt;Exponential-Golomb coding&lt;/a&gt;, or any other universal code.&lt;/p&gt;

&lt;p&gt;Of course it wouldn’t be much of an encoding algorithm if all the steps weren’t reversible! To get back to the initial bit array, we simply iterate over the Medoka encoded array. In our case, if two negative numbers occur one after another, we know that we have to insert the number one there. In other words, the sign periodicity of the “sum_transform” step, allows us to infer where a symbol was deleted.&lt;/p&gt;

&lt;p&gt;The code for decoding:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def medoka_decoding(a,symbol):
  b = []
  for i in range(len(a)-1):
      b.append(a[i])
      if np.sign(a[i]) == np.sign(a[i+1]):
          b.append(symbol)
  b.append(a[-1])
  return b

d = medoka_decoding(c, 1)
print(d)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[-7, 2, -3, 1, -3, 1, -19, 1, -7, 2, -2, 1, -18, 1, -20, 1, -11]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From here we can easily recreate our initial bit array.&lt;/p&gt;

&lt;p&gt;In this simple example, I used only a single symbol for the deletion step of the Medoka encoding scheme. We can technically use however two symbols, one symbol with a positive sign, and another with a negative sign. If two negative numbers repeat, we insert the positive symbol during the decoding step. If two positive numbers repeat, we insert the negative symbol during the decoding step. Notice however that if both symbols occur next to each other, only one of them can get deleted, otherwise the computational steps become irreversible.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/medoka-encoding</link>
        <guid isPermaLink="true">http://localhost:4000/medoka-encoding</guid>
        
        <category>article</category>
        
        <category>compression</category>
        
        <category>bitmap</category>
        
        
      </item>
    
      <item>
        <title>The Engram</title>
        <description>&lt;p&gt;This post is part of an ongoing online book. To access the other parts, please refer to the contents page of the &lt;a href=&quot;https://lums.blog/The-Competitive-Neuron&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The brain is unbelievably complex. Walking in a park, having a conversation with your friends, understanding what you’re reading, hearing, being aware of others’ feelings, these are enormously complex processes. They might seem trivial to us, but when looking at them from a machine’s perspective, we can see how impossibly difficult they really are. All that is somehow governed by a huge cluster of tiny cells in our heads, also known as neurons. A neuron in itself does not ‘‘know’’ anything, it is just a biological machine, but somehow a bunch of neurons together form a consciousness. This, is one of the greatest unsolved mysteries of our time.&lt;/p&gt;

&lt;p&gt;It seems like the mind is an emergent property of the brain. “Emergence” is when a system has properties which its parts do not have, but emerge because of rules, or interactions between its parts. Let’s take an ant colony as an example. An ant is pretty dumb. Its decision making abilities are limited, and individually it can’t plan anything ahead. Yet an ant colony is enormously complex and smart. Ants in a colony have different “jobs”, perform various collective tasks, and live in self-made structures that are way too complex for the mental capability of a single ant. One could arguably call an ant colony an organism in itself. It can make complex decisions as if it were a single entity, yet under the hood it’s all governed by a decentralised system of tiny, almost brainless ants. Our brains are not much different in that regard. The brain is a collection of dumb machines, that somehow together do very smart things.&lt;/p&gt;

&lt;p&gt;Parts in an emergent system often follow very simple rules. When those parts interact with one another, complexity can arise. During the evolution of the nervous system, many mental algorithms emerged, each from simple neuronal interactions, and each giving organisms a fascinating ability. The ability to form memories, to imagine, to infer the actions of another organism, all these abilities are derived from emergent algorithms that “run on top” of neural circuits. When talking about ‘‘algorithms’’, we should not imagine a software running on a computer. Even a vending machine can be viewed as an algorithm. You put coins into the machine, select an item, and it will perform a specific sequence of steps. Procedures, that’s all what algorithms are. In the case of the mind, the neural circuits in our head are what enable such procedures to emerge.&lt;/p&gt;

&lt;p&gt;Memory, induction, the ability to detect movement, the ability to detect the  passage of time, are all emergent algorithms. Even more complex processes, such as deduction, creativity, or imagination fall in the same realm, all these processes somehow emerge from neural circuits. Changes in these neural circuits or neural structures, can result in changes to an algorithm, or in some cases even create a new algorithm. If a useful algorithm appears due to a mutation in one of the neural circuits, that organism will have a competitive advantage over other organisms. Evolutionary forces can then shape the circuits and the neuronal structures over time, in a way that makes the emergent algorithm more efficient.&lt;/p&gt;

&lt;p&gt;This is precisely why I argue that it is crucial to understand the biology of the brain. If an emergent algorithm significantly helps an organism survive, and as a result enables the organism to have on average more offspring, then any organism with a more optimised circuitry  for the emergent algorithm, will have a bigger advantage over other organisms. This might sound over complicated at first, but what this means, is that over a long time, evolutionary forces will shape the underlying neural circuitry in a way that optimises for the emergent algorithms. This means that it should be possible to reverse engineer the emergent algorithms, by looking at the optimised physical structures and biochemical processes of neural circuits.&lt;/p&gt;

&lt;p&gt;Always remember: In biology, form follows function. If there is a specific pattern in nature, that repeats over and over again in different species, we can be very confident that the repeating pattern has an important role. By looking at the optimised morphology, structures, and processes of different neurons, we can start to guess their purpose in the context of the mind. Notice how I am not saying “in the context of learning”, but “in the context of the mind”. The ability to learn, i.e. the ability to infer correctly future events based on past experiences, is only one emergent phenomenon in a set of phenomena to which I am referring to as “the mind”. To decipher how the mind works, I argue that we have to treat it as a set of emergent algorithms. The ability to learn, deduce, imagine, memorise, all these processes are emergent algorithms that are part of the mind. I want to emphasise one thing however: I am not claiming that we can understand all emergent-phenomena of the brain by looking at individual neurons. Even if we know a neuron inside out, it doesn’t tell us much about how a group of neurons interact with one-another. A reductionist mindset will not bring us far when trying to decipher the mind. By treating the brain on the other hand as a complex system with several emergent properties, we can start to guess how different parts interconnect with one-another.&lt;/p&gt;

&lt;p&gt;In this book, I am particularly interested to examine the algorithm that gives rise to memory. When I say “memory”, I do not only mean the ability to recall past events. What I mean by it, is the ability to store external, or behavioural information in neurons and neural circuits. Neuroscientists often use different terms for memory, such as explicit memory, or implicit memory, depending on the kind of memory we are talking about. In this book any information that gets stored in neurons, or neural circuits, will simply be referred to as “memory”. Do not get deceived by the term however, memory in the context of the brain is very different from the memory we use in computers. Today’s computers are based on something known as the von Neumann architecture, where we have a CPU (the basic processing unit), and a memory. In this setup, for any kind of processing, the memory has to be moved from the memory unit to the processing unit. This is time consuming and quite energy inefficient. The setup for processing and memory in the brain on the other hand is much more sophisticated. Computing and memory are not distinctly separated in the brain, they go hand in hand. somehow they are co-located in the neural circuits. It is truly mind blowing when you think about it - somehow the mesh of neurons located in our skull, is able to rapidly store external information and also retrieve it shockingly fast and accurately. To this day, we have no idea how the brain does it, but we are getting closer! We know for example that a memory tends to be associated with specific activation of neurons. Each of our experiences and behaviours somehow gets associated with a neural pattern.&lt;/p&gt;

&lt;p&gt;When you smell a flower for example, a specific pattern of neurons in your brain will fire together, giving you that specific sensation. Once that pattern appears, a cascade of other patterns can emerge. The odour of the flower might trigger a specific memory. You might remember a meadow that you once visited as a child. An emotion might emerge in the cascade of patterns, causing you to feel nostalgia. All this because of a single neural pattern that was caused by the odour of a flower. How was that pattern formed? The question is not only about the physical substrate of memory (which we will examine in much detail), but about the coordination of read and write operations as well. How does the brain know how to form patterns, and how does the brain know which patterns to activate in which scenarios, and at which time? Unlike computers, the real brain  is highly asynchronous.&lt;/p&gt;

&lt;p&gt;“Asynchrony” is when events in a system do not occur in a sequential order, but rather during overlapping time periods. We say that something runs “concurrently” when events, or computations can advance without waiting for all other events, or computations to complete. In the case of the brain, the bursts of firing neurons represents the concurrent events. How can brain activities converge into specific neural patterns, when the whole system runs concurrently? If a neuron fires just a bit too early, or a bit too late than the other neurons, the desired pattern might not emerge, which could for example lead to an execution failure of an action down the line. As an example, you might see a ball flying at your direction, fail to process the right action due to a bad timing between neurons in your brain, and get hit by it in the face. The fact that such scenarios do not happen all the time, makes this all the more intriguing. The problem of asynchronous communication in neural circuits is especially tricky, because none of the neurons “know” when the other neurons will fire, but somehow they still manage to coordinate and act as one. Any computer scientists can tell you how difficult it is to build a working, robust asynchronous system. It is in fact a nightmare. And yet here we are, each one of us carrying with us the most complex asynchronous machine in the known universe, and it spends less energy than a light bulb.&lt;/p&gt;

&lt;p&gt;The problem of neural coordination can be categorised as a Byzantine Generals Problem. The Byzantine Generals Problem was first named by Lamport, Shostak, and Pease in a wonderful paper in 1982, which not surprisingly was titled ‘‘&lt;a href=&quot;https://people.eecs.berkeley.edu/~luca/cs174/byzantine.pdf&quot;&gt;The Byzantine Generals Problem&lt;/a&gt;’’. The abstract idea of it is simple: Imagine that you are a general in the byzantine army and are planning to attack an enemy fortress. The fortress is completely surrounded by several of your battalions, each controlled by a different general. If you all manage to attack the fortress at the same time, you will succeed in taking it down. An uncoordinated attack on the other hand, will end in defeat. How do you make sure that all battalions will attack at the same moment? Even though the Byzantine Generals Problem was originally conceived and formalised as a condition in distributed computing systems, we can see how neural circuits share a very similar problem. neurons are more likely to fire when they receive their inputs simultaneously. It seems like right timing is a necessary component for proper pattern formation and activation. As we will see later in later chapters, the temporal aspect of the neural coordination problem plays a key role in memory formation.&lt;/p&gt;

&lt;p&gt;Besides emerging from completely asynchronous processes, memory has also another very interesting property - It is “re-programmable”. Different organisms will have different memories, depending on their experiences. You were not born knowing what an apple is, but once you look at an apple, once you taste one, you will store a mental representation of the apple in your brain. Even more fascinating, next time you see an apple, you will know what it is, even though the apple you had the first time looked slightly different from the new one. The ability to figure out what something is, based on past experiences is also known as inductive reasoning, and it proved to be a very useful tool in the evolution of animals.&lt;/p&gt;

&lt;p&gt;Besides re-programmable memory, living organisms also have hard-coded memories, which evolved out of necessity. Ants for example (and many other social insects) have an interesting imprinted behaviour known as necrophoresis. Whenever there are any dead bodies of other members in the colony nest, or in the highways where the ants travel, the ants will carry the corpses and put them in a pile that is far enough from the colony. Ants never learned this kind of behaviour, they were born with it. In other words, this behaviour is pre-programmed in their genetic code, it did not emerge through experience. We can find even more interesting cases of hard-coded behaviours in honeybees. Honeybees communicate the location of nearby flowers with dance. They perform either “waggle dances”, or “round dances” inside their beehive, in the presence of other bees. The other bees receive the information from the dance, and then fly to the specific flower location. The preciseness of the honeybee language is astonishing, what is even more surprising however, is that it is encoded in their DNA. Different species of bees will have different “dialects” for communicating with one-another. Back in 1995, T. E. RindererL. D. Beaman &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/24169907/&quot;&gt;showed&lt;/a&gt; how certain dance dialects differences followed simple Mendelian rules of inheritance.&lt;/p&gt;

&lt;p&gt;Even us humans have encoded memories. Mammalian infants for example can smell and instinctively reach the mother’s breast after birth. Human babies are even able to automatically open their mouth when something is near their mouth, or to start sucking when something is in their mouth. This kind of behaviour was not taught, the information for it was already encoded in the brain - we were born with it. However, only the most crucial of behaviours can be encoded genetically. Adopting new behaviours by encoding memories in the genetic code of a population’s gene pool is a very slow and inconvenient process for most tasks. A mobile organism will need to assess more situations than a static one, and because of that, it will need to have a much larger pool of  possible behaviours to pick from. Coding all that into a genome becomes quickly infeasible. Infants for example are not born with the ability to see, or perform complex movements, as both these tasks are so complex, that they need to be developed through experience.&lt;/p&gt;

&lt;p&gt;The first multicellular mobile organisms thus needed some kind of hardware module, not only with imprinted procedures but with re-programmable memory as well. Mobile organisms needed a way to store new experiences and behaviours on the go. A key word here is “mobile”. A lot of scientists in fact believe that movement is why organisms evolved brains in the first place. In the race for survival, mobile organisms had to evolve the ability for ever more adaptive and complex movements and survival strategies. This became only possible once re-programmable memory evolved. As in previous biological systems, the hardware and algorithms of these organisms were pre-determined by their DNA. The “software” (or the memory) on the other hand was able to change dynamically within an organism’s lifetime. This was a game changer.  Suddenly the complexity of the behaviours an organism could perform was not bound by the slow, genetic instructions that they carried around with them. Organisms were for the first time able to perform quick adjustments to their own software.&lt;/p&gt;

&lt;p&gt;Amusingly enough, without mobility, the need for a brain also disappears. An interesting example for this is the intriguing life-cycle of the sea squirt. This animal, at the beginning of its life-cycle, lives as tadpole-like larvae with a developed nervous system and the ability to swim. Since the larvae is not capable of feeding itself, it will try to find a nice rock and cement itself on it, where it will live for the rest of its life. Once it settles like this, a fascinating transformation unfolds. The larvae starts to digest all its tadpole-like parts, including its rudimentary little brain, to transform into a developed sea squirt. Since it does not move anymore, it does not need a “brain” to live. There is however also another very interesting link between mobility and re-programmable memory.&lt;/p&gt;

&lt;p&gt;It might sound strange, but our sensory systems also require re-programmable memory.  Let’s take the visual system as an example. For our neurons to be able to make sense of the world, they need to be able to detect edges, shapes, colors and depth from the visual input. That kind of information has to be encoded, i.e. stored somehow by the neurons. That’s where the re-programmable memory comes in. Note again that when using the word “memory”, I am referring to the ability to store external, or behavioral information somehow into neural circuits. Thus for the development of vision, memory is required. What’s even stranger though, is that the feedback from movement is just as important. As long as an organism is immobile, it will not be able to learn how to see. Richard Held and Alan Hein made this clear in a fascinating experiment in 1963 known as the “Kitten Carousel”.&lt;/p&gt;

&lt;p&gt;The researchers placed two very young kittens into a carousel that was ringed in vertical stripes. The kittens were still too young to have a developed visual system. The first kitten was able to walk freely inside the carousel according to its own actions, while the second kitten was riding in a gondola inside the carousel, without the ability to control its movement. Both kittens would receive the same kind of visual stimuli from the attached vertical stripes when moving in the carousel. One would expect the two kittens to develop normal vision, but that was not the case. Only the kitten that was able to walk freely developed normal vision. The kitten that did not move but rode the gondola, never managed to learn how to see properly. Its visual system did not undergo any significant changes like that of the first kitten. It seems like vision is more than just receiving inputs to our eyes. To learn how to see, one requires feedback loops from other signals that an organism generates, such as movement. The end result of all the signals and feedbacks is the encoded memory that enables the ability of vision.&lt;/p&gt;

&lt;p&gt;We are only now starting to understand how the mechanisms for such “re-programmable memory” work. This might sound a bit far-fetched, but we are in fact in a very similar position to where Gregor Mendel, the father of modern genetics was almost two centuries ago, when he proposed his famous unit of inheritance (today known as the gene). Mendel was the first person to notice predictable patterns in the inheritance of traits. After thousands of experiments, back in 1865, Mendel proposed a hypothetical unit for inheriting traits. He called these heredity units “factor”. Back then, people knew that different traits were inheritable, but very few people however bothered to ask &lt;em&gt;how&lt;/em&gt; traits were inherited. It is important to remember that back then, Mendel had absolutely no idea what genes were, what their physical substrate was, or how they worked. These hereditary units were purely hypothetical, but they could explain very well his experimental observations.&lt;/p&gt;

&lt;p&gt;Today, a lot of scientists believe in another such hypothetical unit, a unit not for traits, but for memories. We refer to those hypothetical units as &lt;em&gt;Engrams&lt;/em&gt;, or memory-traces. Mendel’s hypothetical unit served as a storage medium for inheritance, while the hypothetical Engrams serve as a storage medium for cognitive information. Its exact mechanisms and the locations of the Engrams however remain enigmatic. We do know some things at least. Remember how different experiences, such as odours, could activate different patterns of neurons in the brain. We know that assemblies of neurons in our brain are somehow associated with specific experiences. Whenever an experience occurs, the neurons of the associated pattern fire together. Donald Hebb, a famous neuropsychologist, noticed this strange phenomenon, and proposed a marvellous theory in his 1949 book ‘‘&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-70911-1_15&quot;&gt;The Organization of Behaviour&lt;/a&gt;’’. Today Hebb’s proposal is known as Hebbian Theory, or Hebb’s rule and it explains how assemblies of neurons can become Engrams. Hebb stated that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;'’Any two cells or systems of cells that are repeatedly active at the same time will tend to become ‘associated’ so that activity in one facilitates activity in the other.’’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In laymen terms, Hebb’s rule states that “neurons that fire together, wire together”. That over simplified statement is indeed what we observe. If an input to the brain (such as an odour) causes the same pattern of neurons to activate repeaditly together, that pattern will become interassociated - it will start to fire together. Many scientists believe that this pattern “auto-association” is the perfect candidate for the engram. There is now overwhelming evidence that ensembles of neurons play an important role in memory storage. A fascinating &lt;a href=&quot;https://www.cell.com/cell/fulltext/S0092-86741930616-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867419306166%3Fshowall%3Dtrue&quot;&gt;experiment&lt;/a&gt; in 2019 for example, showed how by artificially activating behaviourally relevant ensembles of neurons, one could observe consistent behavioural responses from test animals. In other words, whenever the specific behavioural pattern was artificially activated, the mice performed the behaviour that was associated with the pattern. To achieve the forced pattern activation, the researchers genetically engineered the mice to have neurons covered with special proteins, that when hit by a light-beam cause the neuron to fire. This ingenious technique is also known as optogenetics. What was even more fascinating however, was that some ensembles of neurons had pattern completion properties. By simply artificially activating two of the ensemble neurons, the researchers were able to trigger the entire ensemble to activate. The results of the experiment beautifully coincide with Hebb’s rule.&lt;/p&gt;

&lt;p&gt;In the scientific community, the biological process behind Hebb’s theory is also known as “Spike-timing-dependent plasticity” (STDP). If neuron A activates neuron B, spike-timing-dependent plasticity is the process that adjusts the connection strength between the two neurons, depending on the timing of the activations. Hebb’s statement however, that “Any two cells that are repeatedly active at the same time will tend to become ‘associated’ so that activity in one facilitates activity in the other”, is an observation, not a mechanism. Why do two neurons become active at the same time in the first place? What exactly is happening at a cellular level when neurons fire? Might the process of engram formation actually be more complicated than in the proposed spike-timing-dependent plasticity model? This is what I will aim to answer in the upcoming chapters. We will learn how engrams are really formed and stored.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/the-engram</link>
        <guid isPermaLink="true">http://localhost:4000/the-engram</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>The Competitive Neuron - On the Formation of Memories and Neuronal Specialisation</title>
        <description>&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;h2 id=&quot;introduction&quot;&gt;&lt;a href=&quot;https://lums.blog/introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;part-one---biology&quot;&gt;Part One - Biology&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lums.blog/the-engram&quot;&gt;The Engram&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lums.blog/the-ghost-in-the-neuron&quot;&gt;The Ghost In The Neuron&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;part-two---technology&quot;&gt;Part Two - Technology&lt;/h2&gt;
</description>
        <pubDate>Sun, 03 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/The-Competitive-Neuron</link>
        <guid isPermaLink="true">http://localhost:4000/The-Competitive-Neuron</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>Introduction</title>
        <description>&lt;p&gt;This post is part of an ongoing online book. To access the other parts, please refer to the contents page of the &lt;a href=&quot;https://lums.blog/The-Competitive-Neuron&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Beyond Ghor, there was a city. All its inhabitants were blind. A king with his entourage arrived nearby; he brought his army and camped in the desert. He had a mighty elephant, which he used to increase the people’s awe. The populace became anxious to see the elephant, and some sightless from among this blind community ran like fools to find it. As they did not even know the form or shape of the elephant, they groped sightlessly, gathering information by touching some part of it. Each thought that he knew something, because he could feel a part…. The man whose hand had reached an ear… said: “It is a large, rough thing, wide and broad, like a rug.” And the one who had felt the trunk said: “I have the real facts about it. It is like a straight and hollow pipe, awful and destructive.”. The one who had felt its feet and legs said: “It is mighty and firm, like a pillar.” Each had felt one part out of many. Each had perceived it wrongly….&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Idries Shah - Tales of the Dervishes, 1967&lt;/cite&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This ancient story was told to teach a simple lesson that is often ignored: The behaviour of a system cannot be known just by knowing the elements of which the system is made.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We have come a long way since  Santiago Ramón y Cajal published his first iconic drawings of neurons back in the late 19th century. The sheer amount of discoveries made by scientists since then proves that we are gradually converging on an understanding of how the brain works. The ingenious techniques that neuroscientists are developing to record and analyse our brains, are now helping us illuminate one by one parts of a once unknown world. Whereas the neuronal morphology and the molecular mechanisms of different neural structures are relatively well understood, the bigger picture of how neuronal interactions form emergent phenomena remains enigmatic. More than a century after the neuron’s discovery, we still do not know how the neural circuity in our heads gives rise to the mind. Like the blind men beyond Ghor, we are still trying to make sense of the elephant ourselves.&lt;/p&gt;

&lt;p&gt;Humans in fact tried to understand the nature of the mind for centuries. In ancient India for example, philosophers believed in a theory called “Samskara”. Samskara meant different things to different people, and it was always somehow mixed with religion and mysticism. It represented the mental impressions, or psychological imprints of a person. Samskaras were explained as characteristics, or behavioural traits that one either possessed from the moment of birth, or that got shaped over time. Indian philosophers  of the Nyaya school of Hinduism understood that a newborn child has imprinted memories (even though they did not refer to it like that). They argued that a baby’s instinctive reach for the mother’s breast was a sign that the baby had some prior Samskara. Since no one provided the knowledge of the necessity of the mother’s breast to the baby, and since the baby did not form any samskaras so far, philosophers believed that the newborn’s knowledge came from a ‘‘prior experience’’.&lt;/p&gt;

&lt;p&gt;As always, it was the Ancient Greeks however that hit the nail on its head. Alcmaeon of Croton, one of the greatest minds of Ancient Greece, was the first one to propose that the brain is the organ of the mind. Even though this revelation might not sound like a big deal today, back then it was a revolution in human knowledge. Up until then, it was not that obvious to people that the thinking happens in the brain. The brain was just another organ. Even the father of modern science, Democritus of Abdera, who formulated the atomic theory of the universe, was inspired by Alcmaeon’s discovery. Democritus concurred with Alcmaeon’s discovery and argued that perception is a purely mechanistic (or one might say algorithmic) process. He argued that thinking and feeling were simply features of matter that emerge when organised in a sufficiently fine and complex way and not due to some spirit infused into matter by the gods. During the time of Democritus, where everything was fused with spirituality and mysticism, these were not only bold statements, they were world-shattering. We know that Democritus wrote several books about the mind and senses. Some of the known book titles were “On the Mind”, “On the Senses”, “On Flavours”, “On Colours”, and “On Logic” (book titles back then were still simple). Unfortunately, none of Alcmaeon’s and Democritus’ books survived the passage of time, all we know are the book titles and the references from other philosophers. Who knows where we would have been today, if only we would have managed to preserve the memory of these ancient giants.&lt;/p&gt;

&lt;p&gt;The Ancient Greeks answered the question of ‘‘where’’ the mind takes place. Today’s scientists on the other hand are trying to answer ‘‘how’’ the mind does what it does. How do we learn? Where and how are memories stored? How does consciousness form? These are quite abstract questions, which because of the way they are asked, are difficult to answer. I strongly believe that to answer these questions, we will have to question every neurobiological structure and biochemical process we see. Why is there a very long dendrite that emerges from the cell body of pyramidal cells? Why are so many excitatory neurons covered with dendritic spines? Why are most interneurons spineless? What’s the purpose of the back-propagating signal that occurs inside a neuron? Why do excitatory and inhibitory neurons look so different? What is the purpose of the enigmatic spine apparatus and why do axons also have such a similar organelle?&lt;/p&gt;

&lt;p&gt;I believe that these are the kind of questions that will allow us to reverse engineer the circuits that give rise to the algorithms of our minds. As Albert Szent-Györgyi once said: “If structure does not tell us anything about function, it only means we have not looked at it correctly.” Patterns that repeat over and over again in nature tend to have an important role. In biology after all, form follows function. Instead of focusing on big philosophical questions, I argue that we can deduce and understand the algorithmic parts of the brain by questioning the patterns in the morphology and biochemical processes of neurons. Using a reductionist approach alone however, will not be enough to decipher the mind. a bottom-up approach might generate a lot of facts, but not necessarily new ideas that will lead to discoveries. For that, a holistic mindset is necessary. I am not advocating one mode of thinking over another, on the contrary. We will need to be willing to constantly switch between modes of thinking, to truly tackle complex problems like the mind. As you can quickly notice, one needs to know some basic neurobiology, to understand the questions this book tries to answer. That’s why in the first part of the book, we are going to learn some basic neuroscience.&lt;/p&gt;

&lt;p&gt;The main focus of this book will be about how biological memories form and are stored. The whole first part of the book in fact will deal with that topic. I will describe a concrete algorithm for how memories are formed and stored in neural circuits, and how a phenomena called ‘‘neuronal specialisation’’ can emerge by following very simple rules. My proposed model aims to extend Hebb’s rule, with the necessary temporal attributes for an asynchronous system to function. We will see how neural competition is key for Hebb’s neural assemblies to form. Once we know how this model works, I will also explain what the purpose of some neural structures might be from the context of the mind.&lt;/p&gt;

&lt;p&gt;My interest is not to write a philosophical, vaguely defined proposal for how memories are formed and stored. Instead I will use a more pragmatic approach and explain a concrete, simple to understand process for it. Even though we do not know all the biochemical puzzle pieces for this problem yet, we will see how by looking at many experiments, we can already come up with a simple unsupervised learning algorithm for neural circuits. I feel it is important to emphasise that while designing the model, I tried to be as strict as possible on its biological plausibility, putting neurobiology first and machine learning second. There will be no no unnecessary abstractions that have little to do with actual biology. This book is after all meant to help us get closer to deciphering the brain.&lt;/p&gt;

&lt;p&gt;Instead of immediately trying to answer the big questions, we are going to ask well defined biological questions, like the ones mentioned earlier. We are then going to use the proposed answers to those questions as puzzle pieces for bigger questions, like ‘‘How are memories stored?’’. Often times knowing how to ask the right questions is even more important than the question itself. We will not only examine the proposed model, but the biological structures from which it emerges as well. We will see how many of the questions in neurobiology, including the ones mentioned earlier in this introduction, can be indirectly explained and understood through this model. Moreover, the model that I will propose will also reveal the existence of a mysterious code inside neurons. We will discuss the implications of such a code, as well as the evidence supporting the existence of such a code, in great detail.&lt;/p&gt;

&lt;p&gt;As you have probably seen in the contents page, the first part of the book is called “Biology”, whereas the second part “Technology”. While the first part describes the neurobiological structures and the emergent phenomena of neural circuits, the second part of the book focuses on the potential real-world applications of the proposed model. In the second part of the book, I will discuss about artificial neural networks, as well as the emergent field of neuromorphic computing. We will see what the basic idea of machine learning is and how the concepts of machine learning can be quite a useful mental model to reverse engineer the algorithms of biological systems. I will also explain how todays neural networks work, and why I believe that it is so important to figure out alternatives to the famous backpropagation algorithm. At the end of the second part, we will explore some of the existing neural network architectures and I will propose a new kind of architecture based on the proposed model from the first part of the book.&lt;/p&gt;

&lt;p&gt;Writing this book turned out to be tricky. On the one hand, the book has to be technical enough so that scientists and researchers do not get annoyed, yet it has to be simple and clear enough, so that people from different backgrounds can nonetheless understand everything. Because of this, I decided that any term or concept used after the introduction, be it neuroscience related or machine learning related, has to be explained in advance. My interest is after all to reach as many people as possible. If we want to inspire someone to adopt a new idea after all, the things we write have to be readable. I also hope that this book will blur a bit the lines between fields. The brain is a complex system, meaning that it consists of many components that interact with one another. Complex systems are inherently hard to model. Knowing the components of the system is only one part of the solution, we also have to figure out how those parts interact. To achieve this, I argue that we have to use an interdisciplinary approach. When studying things, we should not only strive for depth of knowledge, but range as well. Reading papers and books from other fields, can foster lateral thinking and help us come up with completely new ideas. Even if the idea that I will present will turn out not to reflect reality, I hope that the facts presented in this book, and the questions asked will help others decipher the algorithms of the mind.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/Introduction</link>
        <guid isPermaLink="true">http://localhost:4000/Introduction</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>Using binary vector clocks to replace Ethereum transaction nonces</title>
        <description>&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;ol id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#contents&quot; id=&quot;markdown-toc-contents&quot;&gt;Contents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-order-of-ethereum-transactions&quot; id=&quot;markdown-toc-the-order-of-ethereum-transactions&quot;&gt;The order of Ethereum Transactions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#partial-orders-and-join-semilattices&quot; id=&quot;markdown-toc-partial-orders-and-join-semilattices&quot;&gt;Partial Orders and Join-Semilattices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-binary-vector-clock&quot; id=&quot;markdown-toc-the-binary-vector-clock&quot;&gt;The Binary Vector Clock&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-inevitable-total-order-during-epoch-jumps&quot; id=&quot;markdown-toc-the-inevitable-total-order-during-epoch-jumps&quot;&gt;The inevitable total order during epoch jumps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;abstract&quot;&gt;abstract&lt;/h1&gt;
&lt;p&gt;In this blog post, I’ll present the idea of a Binary Vector Clock, a simple, yet space-efficient algorithm for generating a partial order of transactions in account-based blockchain systems. The Binary Vector Clock solves the problem of order dependency in systems such as Ethereum, caused by the total order of transactions that come from the same address holder. What that exactly means will become clear in a bit. The proposed algorithm has the same security as using regular transaction nonces, requires very little overhead, and can potentially result in a significant increase in throughput for systems like Ethereum. This paper was originally submitted on the 15th of April, 2020 on &lt;a href=&quot;https://arxiv.org/abs/2004.07087&quot;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;There are generally two kinds of transaction models in blockchains: The UTXO model, and the account based model. The UTXO model was the first transaction model to be proposed and has many intriguing properties. In this paper however, we are going to focus on the account based model, more exactly on the one implemented by Ethereum. In the account based model, instead of having coins as unspent outputs like in the UTXO model, every participating node has an account, or a balance. When a transaction is created, the transaction’s value is simply reduced from the owners account, and added to someone else’s account. To understand the problem that the Binary Vector Clock tries to solve, let’s first look at the structure of an Ethereum transaction and how the &lt;em&gt;order&lt;/em&gt; of transactions is determined.&lt;/p&gt;

&lt;h1 id=&quot;the-order-of-ethereum-transactions&quot;&gt;The order of Ethereum Transactions&lt;/h1&gt;
&lt;p&gt;A transaction in Ethereum is essentially a message that gets signed by an account holder, also known as an externally owned account. Once a transaction gets created, it is broadcast to other nodes in the system, and eventually recorded by the Ethereum blockchain. The structure of an Ethereum transaction consist of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;em&gt;value&lt;/em&gt; (the amount of ether we want to transfer).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;recipient&lt;/em&gt; (the address of the account to whom we want to send the transaction to).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;gas price&lt;/em&gt; (much like a transaction fee. The gas price shows how much of a fee the originator of the transaction is willing to pay).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;gas limit&lt;/em&gt; (the maximum fee that the originator is willing to pay).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;v,r,s&lt;/em&gt; (the three ECDSA digital signature components to prove that the originator truly formed the transaction).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;data&lt;/em&gt; field (an optional field that can contain code, for when an account interacts with smart contracts).&lt;/li&gt;
  &lt;li&gt;And the &lt;em&gt;nonce&lt;/em&gt; (an account specific counter. Whenever a transaction from the address holder gets confirmed, the counter increments).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The nonce field is the field that is of particularly interest to us. The transaction nonce, not to be confused with the block nonce used for Proof of Work, is a scalar value that serves as a counter. The nonce shows the number of confirmed transactions that originated from the account. Having such a counter for each transaction has an interesting effect: It protects the user from transaction duplication. Let’s see what would happen if transactions had no nonce, to better understand why having such a counter is so important: Let’s say Alice sent Bob a completely valid transaction containing three ether. The signature turned out to be truly Alice’s, and the transaction got recorded on the blockchain. Bob however turns out to have a bad moral compass and wants more money. Without a transaction nonce, there is nothing to stop Bob from “replaying” Alice’s transaction, and claim again three ether. Bob could in fact repeat transmitting Alice’s old transaction to the network, until he gets all of Alice’s ether. Every time the transaction would be replayed, nodes in the system would think that it is a new transaction. In reality however, this is not what happens. By having a counter attached to the transaction, every transaction becomes unique. If let’s say Alice’s transaction has a nonce of 42, Bob will not be able to replay that transaction, as any new transaction coming from Alice would have to have a nonce greater than 42.&lt;/p&gt;

&lt;p&gt;There is however also another important reason to have a nonce in an account-based transaction: We want to be able to determine the &lt;em&gt;order&lt;/em&gt; of transactions. Let’s assume this time that Alice is sending two transactions, but the second transaction is dependent on the first one, i.e. running the second transaction before the first one is invalid (for whatever reason). In a centralized system this is no problem, one would simply confirm the first transaction first, and than continue with the second transaction. In a decentralized system however, nodes in the network might receive the second transaction before the first one. We cannot know in advance in which order nodes will perceive events. Without a counter, there would be no way for nodes in the network to tell which transaction comes first. If on the other hand the first transaction has a counter of 42 and the other transaction has the next counter (43), the order can be determined. If a node in the network thus receives the second transaction before the first one, it knows that it should ignore the second transaction, until the first transaction gets confirmed.&lt;/p&gt;

&lt;p&gt;This is a great feature, but it also has its shortcomings. If Alice were to send several transactions one after another, and one of the transactions does not get included in any block for some reason, e.g. the transaction turns out to be invalid, then none of the subsequent transactions get processed. Only after providing a transaction with the missing nonce, do all the other transactions get processed.
This is no problem if every transaction depends on the previous one, but in most real-world applications that would not be the case. Many nodes have to create dozens of transactions in a short period of time, imposing an order dependency thus can result in transactions having to stay in mempools, even if they could have been processed sooner. The total order of transactions represents at the same time a great feature, and a serious scaling problem for account-based transaction models. In the following sections, I will present how we can overcome the problem of total order when processing transactions.&lt;/p&gt;

&lt;h1 id=&quot;partial-orders-and-join-semilattices&quot;&gt;Partial Orders and Join-Semilattices&lt;/h1&gt;
&lt;p&gt;Before jumping straight to how the Binary Vector Clock works, it is necessary to have a good grasp of what a partial order is. All of us intuitively understand the idea of “total orders” - One is smaller than two, five is greater than four, etc. In order theory, a set is said to have a total order, if for any element $a$ and $b$, a comparison is possible, i.e. either $a \leq b$ or $a \geq b$. For example: Every transaction nonce for an address, is comparable to any other transaction nonce for that address. We thus can easily know which transaction happened-before another transaction, thanks to the total order of transactions. But what if it does not matter in which order some of our transactions get confirmed? If eight out of ten transactions generated from an address holder could in fact be confirmed in any desired order, it would be quite wasteful not to do so. This is however what happens in today’s totally ordered account based transaction model.&lt;/p&gt;

&lt;p&gt;It would thus be of enormous interest if we could somehow “capture” the transaction independence for address holders. This is where partial orders become useful. A partially ordered set, is a set in which only certain pairs of elements are comparable, i.e. one element precedes the other in the ordering, but not every pair of elements is necessarily comparable.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot; height=&quot;300&quot; src=&quot;/assets/images/posts/2020/semilattice.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As an example to better understand what a partial order actually is, let’s look at the join-semilattice in figure 1. The diagram shows a set $S$ with eight vectors. We say that an element in $S$ ‘‘happened-before’’ another element, if and only if every value of vector $a$ is less than or equal to every corresponding value in vector $b$. For example: We can conclude that vector $(1,0,0)$ happened before vector $(1,1,0)$, because none of the values in vector $(1,0,0)$ are greater than in vector $(1,1,0)$ - We say that $(1,0,0)$ happened-before $(1,1,0)$. If on the other hand we try to compare vector $(1,1,0)$ and $(1,0,1)$, one can see that both vectors have values larger than the other vector at some indices. We say that this pair is &lt;em&gt;not comparable&lt;/em&gt;. One cannot determine which element occurred before the other one. Algorithms used in distributed systems, such as vector clocks, take advantage of partial orders. In the context of the distributed systems, having incomparable vectors, or “clocks”, usually means that the events occurred concurrently, and thus have no information of one another. In the case of the Binary Vector Clock on the other hand, incomparableness between two transactions does not indicate concurrency, it indicates that they occur &lt;em&gt;independently&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;the-binary-vector-clock&quot;&gt;The Binary Vector Clock&lt;/h1&gt;
&lt;p&gt;Let’s imagine that instead of a nonce (i.e. counter) for a transaction, we have a counter &lt;em&gt;and&lt;/em&gt; a very small bit array (for the sake of a better explanation, let’s stick to three bits, like the vectors in figure 1. Alice’s Binary Vector Clock is initially set to $(0, [0,0,0])$ (where the first element represents the counter and the second element the bit array). For simplicity, I will refer to the Binary Vector Clock from now on as a “timestamp”. Now let’s say Alice wants to send three transactions one after another. Alice however knows that her second transaction is dependent on her first transaction, but her third transaction has no logical dependency to the two first transactions. Having this information, Alice can do something clever: Instead of incrementing her counter for each transaction, she increments one of the bits in her bit array. Let’s say the first transaction has the timestamp $(0,[0,0,1])$, the second transaction has the timestamp  $(0,[0,1,1])$, and the third timestamp is $(0,[1,0,0])$. All three transactions were send one after another to the network. Any validator receiving the transactions can independently know in what order the transactions need to be confirmed (or if any order exists at all). Validators first look at the counter, the counter tells a validator if the transaction is in the right “epoch” (more on that in a bit). If the counter is equal to the previously confirmed transaction from that address, the bit array is checked. As the bit array of the first and third transaction are not comparable (no order can be determined), even if the first transaction turns out to be invalid for some reason, the third transaction can still be processed by the validators. This is because both timestamps are indicating “independentness”, there is no “happened-before” relationship between them. The second and the first transaction on the other hand do have a “happened-before” relationship. When looking at the bit array of the second transaction, we can conclude that it must have happened after the first transaction. If a validator thus would receive the third transaction and the second transaction, but not the first transaction for some reason, it would know that the third transaction can be processed, but the second transaction not, as it depends on a prior transaction (the first transaction). If a transaction gets confirmed, the address’ Binary Vector Clock gets simply added with the newly confirmed timestamp. Taking again the three transactions from the previous scenario as an example, if Alice’s initial timestamp was $(0,[0,0,0])$, and her first and third transactions get confirmed, her new timestamp would be $(0,[1,0,1])$. Once all the bits in the bit array are turned to one, we can increment the timestamp’s counter, and set the bit array to zero again. We call this shift an “epoch”.&lt;/p&gt;

&lt;p&gt;Up to this point, some of the readers might have already thought something in the lines of: But what if Alice has only one ether, and she creates three independent transactions, each spending one ether? It is important to remember that this is an issue only if transactions would be processed concurrently, which is not the case with the Binary Vector Clock technique. In cases like the one mentioned above, transactions would be treated the same way today’s transactions get treated, if they were to have the same nonce. Today, with the nonce approach, if transactions have the same nonce, one of the transactions would get confirmed (depending on the block creator) and the rest of the transactions would become invalid. In the case of the Binary Clock, one of Alice’s transactions (depending on the block creator) would get confirmed, while the rest of the transactions would simply be considered invalid, regardless of their order independency.&lt;/p&gt;

&lt;h1 id=&quot;the-inevitable-total-order-during-epoch-jumps&quot;&gt;The inevitable total order during epoch jumps&lt;/h1&gt;
&lt;p&gt;It is important to note that there is nonetheless an inevitable transaction processing dependency when shifting from one epoch to the next. Transactions from one epoch can only be processed independently, after the transactions of the previous epoch were already processed. In other words if Alice were to send three other transactions one after the other, where the first transaction would have a timestamp of $(0,[1,1,1])$, second transaction $(1,[1,0,0])$, and third transaction $(1,[0,0,1])$, even if all three transactions are completely independent from one another, the second and third transactions will not be able to get processed without the first one being confirmed first. This is because these transactions occurred during an epoch “jump”, i.e. the Binary Vector Clock gets incremented, and the bit array becomes set to zero. The transactions in the new epoch cannot know if they are comparable or not with the transaction from the previous epoch, forcing a momentary total order. I argue however that the space-wise inexpensive nature of the Binary Vector Clock, and its property to handle partial orders, makes it an attractive technique for the account-based transaction model, even in the case of momentary order dependencies between epoch jumps.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In This paper I introduced the Binary Vector Clock, a memory-wise inexpensive partially ordered counter for account-based transactions, that solves the issue of order dependency when processing transactions. Note that the Binary Vector Clock does not suggest the concurrent processing of transactions in Ethereum. Doing so would in fact introduce many possible attack vectors to the system. It only specifies which transactions can be processed independently, and which ones depend on a prior transaction confirmation. If for example an address generates $N$ transactions one after another, and the first transaction fails, the subsequent transactions are still able to get processed and confirmed by the blockchain. This is not the case in today’s approach with transaction nonces. In today’s approach, if the first transaction fails for some reason, all of the other transactions would need to be ignored until the gap in the nonce becomes filled. The Binary Vector Clock overcomes the issue by introducing a partial order between transactions of the same address holder. Using the Binary Vector Clock as a substitution for the transaction nonce gives more freedom to the user in determining transaction orders. The Binary Vector Clock allows the user to specify if a transaction can be processed  independently from other transactions, or if it should be queued until a certain transaction gets confirmed. I argue that this ability has important implications for blockchain systems. Considering that transactions in blockchain systems most likely follow a pareto distribution (the majority of transactions are generated by very few nodes), introducing an inexpensive technique that allows for independent processing of transactions, could potentially increase the scaling capability of Ethereum and other account-based blockchains significantly.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/The-Binary-Vector-Clock</link>
        <guid isPermaLink="true">http://localhost:4000/The-Binary-Vector-Clock</guid>
        
        <category>Paper</category>
        
        <category>Vector Clock</category>
        
        <category>Ethereum</category>
        
        
      </item>
    
  </channel>
</rss>
