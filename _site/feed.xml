<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lum Ramabaja</title>
    <description>Welcome to my blog</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 28 Jun 2020 23:21:25 +0200</pubDate>
    <lastBuildDate>Sun, 28 Jun 2020 23:21:25 +0200</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Using Medoka encoding to compress sparse bitmaps</title>
        <description>&lt;p&gt;In this short article, I am going introduce the idea of Medoka encoding, a simple lossless compression technique which I invented a couple of years ago. In contrast to the &lt;a href=&quot;https://en.wikipedia.org/wiki/LZ77_and_LZ78&quot;&gt;LZ family&lt;/a&gt; of lossless compression algorithms, Medoka encoding can only be applied to bit arrays, aka bitmaps. The technique is unbelievably simple to code, it is highly parallelizable, very quick in both encoding and decoding, and it usually performed better in my experiments than LZ techniques (of course, more rigorous experiments have to be done for that claim).&lt;/p&gt;

&lt;p&gt;Before explaining how the encoding scheme works, let’s look at some simple use cases for bitmap compression:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bitmap_index#Compression&quot;&gt;Bitmap index&lt;/a&gt; compression. Bitmap indices are just bit arrays used in databases to answer queries by performing bitwise logical operations. In some cases however, our data columns can be quite sparse. In other words our bitmap would have a lot of zeros, and just a few ones. When dealing with such cases, we usually try to compress the bitmap, so as to not waste unnecessarily space.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bloom_filter&quot;&gt;Bloom Filters&lt;/a&gt;. Bloom filters are probabilistic data structures used to test whether an element is a member of a set. They are probabilistic, because false positives can occur while checking for the presence of elements. Bloom filters are also just bit arrays. The ratio of ones and zeros in the bloom filter depends on the chosen parameters of the bloom filter, as well as on the number of elements inserted to the bloom filter. If the difference between zeros and ones is large, it’s better to compress the bloom filter, before sending it to another node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s important to note that in both mentioned cases, if we were to use a compression technique such as LZ77, we would have first to decode the data, before being able to query it. Medoka encoding does not have this problem, one  can directly participate in bitwise operations without decompressing the data.&lt;/p&gt;

&lt;p&gt;Now let’s look at how Medoka encoding works, by following simple Python code:&lt;/p&gt;

&lt;p&gt;First let’s create a random sparse list with ones and zeros:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
a = (np.random.rand(100) &amp;lt; 0.1).astype(int).tolist()
print(a)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output will look something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To perform Medoka encoding, we first need to transform the bit array. For the sake of it, let’s call the transformation “sum_transform”:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def sum_transform(a):
  mapp = {1:1, 0:-1}
  s = []
  n = 1
  prev = a[0]
  for i in a[1:]:
      if i == prev:
          n += 1
      else:
          s.append(n * mapp[prev])
          n = 1
          prev = i
  s.append(n * mapp[prev])
  return s

b = sum_transform(a)
print(b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[-7, 2, -3, 1, -3, 1, -19, 1, -7, 2, -2, 1, -18, 1, -20, 1, -11]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we can see, “sum_transform” takes as input the bit list. To better understand what we’re actually doing at this step, look at both the bit list output, as well as the sum_transform output. The function sums up all the zeros that are next to each other and assigns a negative sign to that integer. It does the same thing to the ones, and assigns a positive sign to the integer.&lt;/p&gt;

&lt;p&gt;Now comes the final step of the Medoka encoding algorithm:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def medoka_encoding(a, symbol):
    b = [a[0]]
    for elm in a[1:-1]:
        if elm != symbol:
            b.append(elm)
    b.append(a[-1])
    return b

c = medoka_encoding(b, 1)
print(c)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[-7, 2, -3, -3, -19, -7, 2, -2, -18, -20, -11]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the final step of the Medoka encoding algorithm, we simply specify the most frequent symbol from out “sum_transform step”, which was the number one. We then remove all ones from sum_transform list. What we get at the end is our final array, that is much more compressable now. We can then compress the array either by using &lt;a href=&quot;https://en.wikipedia.org/wiki/Huffman_coding&quot;&gt;Huffman coding&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential-Golomb_coding&quot;&gt;Exponential-Golomb coding&lt;/a&gt;, or any other universal code.&lt;/p&gt;

&lt;p&gt;Of course it wouldn’t be much of an encoding algorithm if all the steps weren’t reversible! To get back to the initial bit array, we simply iterate over the Medoka encoded array. In our case, if two negative numbers occur one after another, we know that we have to insert the number one there. In other words, the sign periodicity of the “sum_transform” step, allows us to infer where a symbol was deleted.&lt;/p&gt;

&lt;p&gt;The code for decoding:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def medoka_decoding(a,symbol):
  b = []
  for i in range(len(a)-1):
      b.append(a[i])
      if np.sign(a[i]) == np.sign(a[i+1]):
          b.append(symbol)
  b.append(a[-1])
  return b

d = medoka_decoding(c, 1)
print(d)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[-7, 2, -3, 1, -3, 1, -19, 1, -7, 2, -2, 1, -18, 1, -20, 1, -11]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From here we can easily recreate our initial bit array.&lt;/p&gt;

&lt;p&gt;In this simple example, I used only a single symbol for the deletion step of the Medoka encoding scheme. We can technically use however two symbols, one symbol with a positive sign, and another with a negative sign. If two negative numbers repeat, we insert the positive symbol during the decoding step. If two positive numbers repeat, we insert the negative symbol during the decoding step. Notice however that if both symbols occur next to each other, only one of them can get deleted, otherwise the computational steps become irreversible.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/medoka-encoding</link>
        <guid isPermaLink="true">http://localhost:4000/medoka-encoding</guid>
        
        <category>article</category>
        
        <category>compression</category>
        
        <category>bitmap</category>
        
        
      </item>
    
      <item>
        <title>The Engram</title>
        <description>&lt;p&gt;This post is part of an ongoing online book. To access the other parts, please refer to the contents page of the &lt;a href=&quot;https://lums.blog/The-Competitive-Neuron&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The brain is unbelievably complex. Walking in a park, having a conversation with your friends, understanding what you’re reading, hearing, being aware of others’ feelings, these are enormously complex processes. They might seem trivial to us, but when looking at them from a machine’s perspective, we can see how impossibly difficult they really are. All that is somehow governed by a huge cluster of tiny cells in our heads, also known as neurons. A neuron in itself does not ‘‘know’’ anything, it is just a biological machine, but somehow a bunch of neurons together form a consciousness. This, is one of the greatest unsolved mysteries of our time.&lt;/p&gt;

&lt;p&gt;It seems like the mind is an emergent property of the brain. “Emergence” is when a system has properties which its parts do not have, but emerge because of rules, or interactions between its parts. Let’s take an ant colony as an example. An ant is pretty dumb. Its decision making abilities are limited, and individually it can’t plan anything ahead. Yet an ant colony is enormously complex and smart. Ants in a colony have different “jobs”, perform various collective tasks, and live in self-made structures that are way too complex for the mental capability of a single ant. One could arguably call an ant colony an organism in itself. It can make complex decisions as if it were a single entity, yet under the hood it’s all governed by a decentralised system of tiny, almost brainless ants. Our brains are not much different in that regard. The brain is a collection of dumb machines, that somehow together do very smart things.&lt;/p&gt;

&lt;p&gt;Parts in an emergent system often follow very simple rules. When those parts interact with one another, complexity can arise. During the evolution of the nervous system, many mental algorithms emerged, each from simple neuronal interactions, and each giving organisms a fascinating ability. The ability to form memories, to imagine, to infer the actions of another organism, all these abilities are derived from emergent algorithms that “run on top” of neural circuits. When talking about ‘‘algorithms’’, we should not imagine a software running on a computer. Even a vending machine can be viewed as an algorithm. You put coins into the machine, select an item, and it will perform a specific sequence of steps. Procedures, that’s all what algorithms are. In the case of the mind, the neural circuits in our head are what enable such procedures to emerge.&lt;/p&gt;

&lt;p&gt;Memory, induction, the ability to detect movement, the ability to detect the  passage of time, are all emergent algorithms. Even more complex processes, such as deduction, creativity, or imagination fall in the same realm, all these processes somehow emerge from neural circuits. Changes in these neural circuits or neural structures, can result in changes to an algorithm, or in some cases even create a new algorithm. If a useful algorithm appears due to a mutation in one of the neural circuits, that organism will have a competitive advantage over other organisms. Evolutionary forces can then shape the circuits and the neuronal structures over time, in a way that makes the emergent algorithm more efficient.&lt;/p&gt;

&lt;p&gt;This is precisely why I argue that it is crucial to understand the biology of the brain. If an emergent algorithm significantly helps an organism survive, and as a result enables the organism to have on average more offspring, then any organism with a more optimised circuitry  for the emergent algorithm, will have a bigger advantage over other organisms. This might sound over complicated at first, but what this means, is that over a long time, evolutionary forces will shape the underlying neural circuitry in a way that optimises for the emergent algorithms. This means that it should be possible to reverse engineer the emergent algorithms, by looking at the optimised physical structures and biochemical processes of neural circuits.&lt;/p&gt;

&lt;p&gt;Always remember: In biology, form follows function. If there is a specific pattern in nature, that repeats over and over again in different species, we can be very confident that the repeating pattern has an important role. By looking at the optimised morphology, structures, and processes of different neurons, we can start to guess their purpose in the context of the mind. Notice how I am not saying “in the context of learning”, but “in the context of the mind”. The ability to learn, i.e. the ability to infer correctly future events based on past experiences, is only one emergent phenomenon in a set of phenomena to which I am referring to as “the mind”. To decipher how the mind works, I argue that we have to treat it as a set of emergent algorithms. The ability to learn, deduce, imagine, memorise, all these processes are emergent algorithms that are part of the mind. I want to emphasise one thing however: I am not claiming that we can understand all emergent-phenomena of the brain by looking at individual neurons. Even if we know a neuron inside out, it doesn’t tell us much about how a group of neurons interact with one-another. A reductionist mindset will not bring us far when trying to decipher the mind. By treating the brain on the other hand as a complex system with several emergent properties, we can start to guess how different parts interconnect with one-another.&lt;/p&gt;

&lt;p&gt;In this book, I am particularly interested to examine the algorithm that gives rise to memory. When I say “memory”, I do not only mean the ability to recall past events. What I mean by it, is the ability to store external, or behavioural information in neurons and neural circuits. Neuroscientists often use different terms for memory, such as explicit memory, or implicit memory, depending on the kind of memory we are talking about. In this book any information that gets stored in neurons, or neural circuits, will simply be referred to as “memory”. Do not get deceived by the term however, memory in the context of the brain is very different from the memory we use in computers. Today’s computers are based on something known as the von Neumann architecture, where we have a CPU (the basic processing unit), and a memory. In this setup, for any kind of processing, the memory has to be moved from the memory unit to the processing unit. This is time consuming and quite energy inefficient. The setup for processing and memory in the brain on the other hand is much more sophisticated. Computing and memory are not distinctly separated in the brain, they go hand in hand. somehow they are co-located in the neural circuits. It is truly mind blowing when you think about it - somehow the mesh of neurons located in our skull, is able to rapidly store external information and also retrieve it shockingly fast and accurately. To this day, we have no idea how the brain does it, but we are getting closer! We know for example that a memory tends to be associated with specific activation of neurons. Each of our experiences and behaviours somehow gets associated with a neural pattern.&lt;/p&gt;

&lt;p&gt;When you smell a flower for example, a specific pattern of neurons in your brain will fire together, giving you that specific sensation. Once that pattern appears, a cascade of other patterns can emerge. The odour of the flower might trigger a specific memory. You might remember a meadow that you once visited as a child. An emotion might emerge in the cascade of patterns, causing you to feel nostalgia. All this because of a single neural pattern that was caused by the odour of a flower. How was that pattern formed? The question is not only about the physical substrate of memory (which we will examine in much detail), but about the coordination of read and write operations as well. How does the brain know how to form patterns, and how does the brain know which patterns to activate in which scenarios, and at which time? Unlike computers, the real brain  is highly asynchronous.&lt;/p&gt;

&lt;p&gt;“Asynchrony” is when events in a system do not occur in a sequential order, but rather during overlapping time periods. We say that something runs “concurrently” when events, or computations can advance without waiting for all other events, or computations to complete. In the case of the brain, the bursts of firing neurons represents the concurrent events. How can brain activities converge into specific neural patterns, when the whole system runs concurrently? If a neuron fires just a bit too early, or a bit too late than the other neurons, the desired pattern might not emerge, which could for example lead to an execution failure of an action down the line. As an example, you might see a ball flying at your direction, fail to process the right action due to a bad timing between neurons in your brain, and get hit by it in the face. The fact that such scenarios do not happen all the time, makes this all the more intriguing. The problem of asynchronous communication in neural circuits is especially tricky, because none of the neurons “know” when the other neurons will fire, but somehow they still manage to coordinate and act as one. Any computer scientists can tell you how difficult it is to build a working, robust asynchronous system. It is in fact a nightmare. And yet here we are, each one of us carrying with us the most complex asynchronous machine in the known universe, and it spends less energy than a light bulb.&lt;/p&gt;

&lt;p&gt;The problem of neural coordination can be categorised as a Byzantine Generals Problem. The Byzantine Generals Problem was first named by Lamport, Shostak, and Pease in a wonderful paper in 1982, which not surprisingly was titled ‘‘&lt;a href=&quot;https://people.eecs.berkeley.edu/~luca/cs174/byzantine.pdf&quot;&gt;The Byzantine Generals Problem&lt;/a&gt;’’. The abstract idea of it is simple: Imagine that you are a general in the byzantine army and are planning to attack an enemy fortress. The fortress is completely surrounded by several of your battalions, each controlled by a different general. If you all manage to attack the fortress at the same time, you will succeed in taking it down. An uncoordinated attack on the other hand, will end in defeat. How do you make sure that all battalions will attack at the same moment? Even though the Byzantine Generals Problem was originally conceived and formalised as a condition in distributed computing systems, we can see how neural circuits share a very similar problem. neurons are more likely to fire when they receive their inputs simultaneously. It seems like right timing is a necessary component for proper pattern formation and activation. As we will see later in later chapters, the temporal aspect of the neural coordination problem plays a key role in memory formation.&lt;/p&gt;

&lt;p&gt;Besides emerging from completely asynchronous processes, memory has also another very interesting property - It is “re-programmable”. Different organisms will have different memories, depending on their experiences. You were not born knowing what an apple is, but once you look at an apple, once you taste one, you will store a mental representation of the apple in your brain. Even more fascinating, next time you see an apple, you will know what it is, even though the apple you had the first time looked slightly different from the new one. The ability to figure out what something is, based on past experiences is also known as inductive reasoning, and it proved to be a very useful tool in the evolution of animals.&lt;/p&gt;

&lt;p&gt;Besides re-programmable memory, living organisms also have hard-coded memories, which evolved out of necessity. Ants for example (and many other social insects) have an interesting imprinted behaviour known as necrophoresis. Whenever there are any dead bodies of other members in the colony nest, or in the highways where the ants travel, the ants will carry the corpses and put them in a pile that is far enough from the colony. Ants never learned this kind of behaviour, they were born with it. In other words, this behaviour is pre-programmed in their genetic code, it did not emerge through experience. We can find even more interesting cases of hard-coded behaviours in honeybees. Honeybees communicate the location of nearby flowers with dance. They perform either “waggle dances”, or “round dances” inside their beehive, in the presence of other bees. The other bees receive the information from the dance, and then fly to the specific flower location. The preciseness of the honeybee language is astonishing, what is even more surprising however, is that it is encoded in their DNA. Different species of bees will have different “dialects” for communicating with one-another. Back in 1995, T. E. RindererL. D. Beaman &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/24169907/&quot;&gt;showed&lt;/a&gt; how certain dance dialects differences followed simple Mendelian rules of inheritance.&lt;/p&gt;

&lt;p&gt;Even us humans have encoded memories. Mammalian infants for example can smell and instinctively reach the mother’s breast after birth. Human babies are even able to automatically open their mouth when something is near their mouth, or to start sucking when something is in their mouth. This kind of behaviour was not taught, the information for it was already encoded in the brain - we were born with it. However, only the most crucial of behaviours can be encoded genetically. Adopting new behaviours by encoding memories in the genetic code of a population’s gene pool is a very slow and inconvenient process for most tasks. A mobile organism will need to assess more situations than a static one, and because of that, it will need to have a much larger pool of  possible behaviours to pick from. Coding all that into a genome becomes quickly infeasible. Infants for example are not born with the ability to see, or perform complex movements, as both these tasks are so complex, that they need to be developed through experience.&lt;/p&gt;

&lt;p&gt;The first multicellular mobile organisms thus needed some kind of hardware module, not only with imprinted procedures but with re-programmable memory as well. Mobile organisms needed a way to store new experiences and behaviours on the go. A key word here is ‘‘mobile’’. A lot of scientists in fact believe that movement is why organisms evolved brains in the first place. In the race for survival, mobile organisms had to evolve the ability for ever more adaptive and complex movements and survival strategies. This became only possible once re-programmable memory evolved. As in previous biological systems, the hardware and algorithms of these organisms were pre-determined by their DNA. The “software” (or the memory) on the other hand was able to change dynamically within an organism’s lifetime. This was a game changer.  Suddenly the complexity of the behaviours an organism could perform was not bound by the slow, genetic instructions that they carried around with them. Organisms were for the first time able to perform quick adjustments to their own software.&lt;/p&gt;

&lt;p&gt;We are only now starting to understand how the mechanisms for such “re-programmable memory” work. This might sound a bit far-fetched, but we are in fact in a very similar position to where Gregor Mendel, the father of modern genetics was almost two centuries ago, when he proposed his famous unit of inheritance (today known as the gene). Mendel was the first person to notice predictable patterns in the inheritance of traits. After thousands of experiments, back in 1865, Mendel proposed a hypothetical unit for inheriting traits. He called these heredity units “factor”. Back then, people knew that different traits were inheritable, but very few people however bothered to ask &lt;em&gt;how&lt;/em&gt; traits were inherited. It is important to remember that back then, Mendel had absolutely no idea what genes were, what their physical substrate was, or how they worked. These hereditary units were purely hypothetical, but they could explain very well his experimental observations.&lt;/p&gt;

&lt;p&gt;Today, a lot of scientists believe in another such hypothetical unit, a unit not for traits, but for memories. We refer to those hypothetical units as &lt;em&gt;Engrams&lt;/em&gt;, or memory-traces. Mendel’s hypothetical unit served as a storage medium for inheritance, while the hypothetical Engrams serve as a storage medium for cognitive information. Its exact mechanisms and the locations of the Engrams however remain enigmatic. We do know some things at least. Remember how different experiences, such as odours, could activate different patterns of neurons in the brain. We know that assemblies of neurons in our brain are somehow associated with specific experiences. Whenever an experience occurs, the neurons of the associated pattern fire together. Donald Hebb, a famous neuropsychologist, noticed this strange phenomenon, and proposed a marvellous theory in his 1949 book ‘‘&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-70911-1_15&quot;&gt;The Organization of Behaviour&lt;/a&gt;’’. Today Hebb’s proposal is known as Hebbian Theory, or Hebb’s rule and it explains how assemblies of neurons can become Engrams. Hebb stated that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;'’Any two cells or systems of cells that are repeatedly active at the same time will tend to become ‘associated’ so that activity in one facilitates activity in the other.’’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In laymen terms, Hebb’s rule states that “neurons that fire together, wire together”. That over simplified statement is indeed what we observe. If an input to the brain (such as an odour) causes the same pattern of neurons to activate repeaditly together, that pattern will become interassociated - it will start to fire together. Many scientists believe that this pattern “auto-association” is the perfect candidate for the engram. There is now overwhelming evidence that ensembles of neurons play an important role in memory storage. A fascinating &lt;a href=&quot;https://www.cell.com/cell/fulltext/S0092-86741930616-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867419306166%3Fshowall%3Dtrue&quot;&gt;experiment&lt;/a&gt; in 2019 for example, showed how by artificially activating behaviourally relevant ensembles of neurons, one could observe consistent behavioural responses from test animals. In other words, whenever the specific behavioural pattern was artificially activated, the mice performed the behaviour that was associated with the pattern. To achieve the forced pattern activation, the researchers genetically engineered the mice to have neurons covered with special proteins, that when hit by a light-beam cause the neuron to fire. This ingenious technique is also known as optogenetics. What was even more fascinating however, was that some ensembles of neurons had pattern completion properties. By simply artificially activating two of the ensemble neurons, the researchers were able to trigger the entire ensemble to activate. The results of the experiment beautifully coincide with Hebb’s rule.&lt;/p&gt;

&lt;p&gt;In the scientific community, the biological process behind Hebb’s theory is also known as “Spike-timing-dependent plasticity” (STDP). If neuron A activates neuron B, spike-timing-dependent plasticity is the process that adjusts the connection strength between the two neurons, depending on the timing of the activations. Hebb’s statement however, that “Any two cells that are repeatedly active at the same time will tend to become ‘associated’ so that activity in one facilitates activity in the other”, is an observation, not a mechanism. Why do two neurons become active at the same time in the first place? What exactly is happening at a cellular level when neurons fire? Might the process of engram formation actually be more complicated than in the proposed spike-timing-dependent plasticity model? This is what I will aim to answer in the upcoming chapters. We will learn how engrams are really formed and stored.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/the-engram</link>
        <guid isPermaLink="true">http://localhost:4000/the-engram</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>The Competitive Neuron - On the Formation of Memories and Neuronal Specialisation</title>
        <description>&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;h2 id=&quot;introduction&quot;&gt;&lt;a href=&quot;https://lums.blog/introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;part-one---biology&quot;&gt;Part One - Biology&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lums.blog/the-engram&quot;&gt;The Engram&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The Ghost In The Neuron&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;part-two---technology&quot;&gt;Part Two - Technology&lt;/h2&gt;
</description>
        <pubDate>Sun, 03 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/The-Competitive-Neuron</link>
        <guid isPermaLink="true">http://localhost:4000/The-Competitive-Neuron</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>Introduction</title>
        <description>&lt;p&gt;This post is part of an ongoing online book. To access the other parts, please refer to the contents page of the &lt;a href=&quot;https://lums.blog/The-Competitive-Neuron&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Beyond Ghor, there was a city. All its inhabitants were blind. A king with his entourage arrived nearby; he brought his army and camped in the desert. He had a mighty elephant, which he used to increase the people’s awe. The populace became anxious to see the elephant, and some sightless from among this blind community ran like fools to find it. As they did not even know the form or shape of the elephant, they groped sightlessly, gathering information by touching some part of it. Each thought that he knew something, because he could feel a part…. The man whose hand had reached an ear… said: “It is a large, rough thing, wide and broad, like a rug.” And the one who had felt the trunk said: “I have the real facts about it. It is like a straight and hollow pipe, awful and destructive.”. The one who had felt its feet and legs said: “It is mighty and firm, like a pillar.” Each had felt one part out of many. Each had perceived it wrongly….&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Idries Shah - Tales of the Dervishes, 1967&lt;/cite&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This ancient story was told to teach a simple lesson that is often ignored: The behaviour of a system cannot be known just by knowing the elements of which the system is made.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We have come a long way since  Santiago Ramón y Cajal published his first iconic drawings of neurons back in the late 19th century. The sheer amount of discoveries made by scientists since then proves that we are gradually converging on an understanding of how the brain works. The ingenious techniques that neuroscientists are developing to record and analyse our brains, are now helping us illuminate one by one parts of a once unknown world. Whereas the neuronal morphology and the molecular mechanisms of different neural structures are relatively well understood, the bigger picture of how neuronal interactions form emergent phenomena remains enigmatic. More than a century after the neuron’s discovery, we still do not know how the neural circuity in our heads gives rise to the mind. Like the blind men beyond Ghor, we are still trying to make sense of the elephant ourselves.&lt;/p&gt;

&lt;p&gt;Humans in fact tried to understand the nature of the mind for centuries. In ancient India for example, philosophers believed in a theory called “Samskara”. Samskara meant different things to different people, and it was always somehow mixed with religion and mysticism. It represented the mental impressions, or psychological imprints of a person. Samskaras were explained as characteristics, or behavioural traits that one either possessed from the moment of birth, or that got shaped over time. Indian philosophers  of the Nyaya school of Hinduism understood that a newborn child has imprinted memories (even though they did not refer to it like that). They argued that a baby’s instinctive reach for the mother’s breast was a sign that the baby had some prior Samskara. Since no one provided the knowledge of the necessity of the mother’s breast to the baby, and since the baby did not form any samskaras so far, philosophers believed that the newborn’s knowledge came from a ‘‘prior experience’’.&lt;/p&gt;

&lt;p&gt;As always, it was the Ancient Greeks however that hit the nail on its head. Alcmaeon of Croton, one of the greatest minds of Ancient Greece, was the first one to propose that the brain is the organ of the mind. Even though this revelation might not sound like a big deal today, back then it was a revolution in human knowledge. Up until then, it was not that obvious to people that the thinking happens in the brain. The brain was just another organ. Even the father of modern science, Democritus of Abdera, who formulated the atomic theory of the universe, was inspired by Alcmaeon’s discovery. Democritus concurred with Alcmaeon’s discovery and argued that perception is a purely mechanistic (or one might say algorithmic) process. He argued that thinking and feeling were simply features of matter that emerge when organised in a sufficiently fine and complex way and not due to some spirit infused into matter by the gods. During the time of Democritus, where everything was fused with spirituality and mysticism, these were not only bold statements, they were world-shattering. We know that Democritus wrote several books about the mind and senses. Some of the known book titles were “On the Mind”, “On the Senses”, “On Flavours”, “On Colours”, and “On Logic” (book titles back then were still simple). Unfortunately, none of Alcmaeon’s and Democritus’ books survived the passage of time, all we know are the book titles and the references from other philosophers. Who knows where we would have been today, if only we would have managed to preserve the memory of these ancient giants.&lt;/p&gt;

&lt;p&gt;The Ancient Greeks answered the question of ‘‘where’’ the mind takes place. Today’s scientists on the other hand are trying to answer ‘‘how’’ the mind does what it does. How do we learn? Where and how are memories stored? How does consciousness form? These are quite abstract questions, which because of the way they are asked, are difficult to answer. I strongly believe that to answer these questions, we will have to question every neurobiological structure and biochemical process we see. Why is there a very long dendrite that emerges from the cell body of pyramidal cells? Why are so many excitatory neurons covered with dendritic spines? Why are most interneurons spineless? What’s the purpose of the back-propagating signal that occurs inside a neuron? Why do excitatory and inhibitory neurons look so different? What is the purpose of the enigmatic spine apparatus and why do axons also have such a similar organelle?&lt;/p&gt;

&lt;p&gt;I believe that these are the kind of questions that will allow us to reverse engineer the circuits that give rise to the algorithms of our minds. Patterns that repeat over and over again in nature tend to have an important role. In biology after all, form follows function. Instead of focusing on big philosophical questions, I argue that we can deduce and understand the algorithmic parts of the brain by questioning the patterns in the morphology and biochemical processes of neurons. As you can quickly notice, one needs to know some basic neurobiology, to understand the questions this book tries to answer. That’s why in the first part of the book, we are going to learn some basic neuroscience.&lt;/p&gt;

&lt;p&gt;The main focus of this book will be about how biological memories form and are stored. The whole first part of the book in fact will deal with that topic. I will describe a concrete algorithm for how memories are formed and stored in neural circuits, and how a phenomena called ‘‘neuronal specialisation’’ can emerge by following very simple rules. My proposed model aims to extend Hebb’s rule, with the necessary temporal attributes for an asynchronous system to function. We will see how neural competition is key for Hebb’s neural assemblies to form. Once we know how this model works, I will also explain what the purpose of some neural structures might be from the context of the mind.&lt;/p&gt;

&lt;p&gt;My interest is not to write a philosophical, vaguely defined proposal for how memories are formed and stored. Instead I will use a more pragmatic approach and explain a concrete, simple to understand process for it. Even though we do not know all the biochemical puzzle pieces for this problem yet, we will see how by looking at many experiments, we can already come up with a simple unsupervised learning algorithm for neural circuits. I feel it is important to emphasise that while designing the model, I tried to be as strict as possible on its biological plausibility, putting neurobiology first and machine learning second. There will be no complicated math, no unnecessary abstractions that have little to do with actual biology, nothing of that sort. This book is after all meant to help us get closer to deciphering the brain. Not that it is bad to write things in a mathematical way, on the contrary, mathematics is after all the language of the universe. But I feel that too many times, especially in academia, we tend to use mathematics to make things appear more complicated, rather than to simplify them. I will hopefully use the latter approach in this book.&lt;/p&gt;

&lt;p&gt;Instead of immediately trying to answer the big questions, we are going to ask well defined biological questions, like the ones mentioned earlier. We are then going to use the proposed answers to those questions as puzzle pieces for bigger questions, like ‘‘How are memories stored?’’. Often times knowing how to ask the right questions is even more important than the question itself. We will not only examine the proposed model, but the biological structures from which it emerges as well. We will see how many of the questions in neurobiology, including the ones mentioned earlier in this introduction, can be indirectly explained and understood through this model.&lt;/p&gt;

&lt;p&gt;As you have probably seen in the contents page, the first part of the book is called “Biology”, whereas the second part “Technology”. While the first part describes the neurobiological structures and the emergent phenomena of neural circuits, the second part of the book focuses on the potential real-world applications of the proposed model. In the second part of the book, I will discuss about artificial neural networks, as well as the emergent field of neuromorphic computing. We will see what the basic idea of machine learning is and how the concepts of machine learning can be quite a useful mental model to reverse engineer the algorithms of biological systems. I will also explain how todays neural networks work, and why I believe that it is so important to figure out alternatives to the backpropagation algorithm. At the end of the second part, we will explore some of the existing neural network architectures and I will propose a new kind of architecture based on the proposed model from the first part of the book.&lt;/p&gt;

&lt;p&gt;Writing this book turned out to be tricky. On the one hand, the book has to be technical enough so that scientists and researchers do not get annoyed, yet it has to be simple and clear enough, so that people from different backgrounds can nonetheless understand everything. Because of this, I decided that any term or concept used after the introduction, be it neuroscience related or machine learning related, has to be explained in advance. My interest is after all to reach as many people as possible. Even if my hypothesis turns out not to reflect reality, I hope that the facts and the questions asked in this book will help others decipher the mind.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/Introduction</link>
        <guid isPermaLink="true">http://localhost:4000/Introduction</guid>
        
        <category>Book</category>
        
        <category>Neuroscience</category>
        
        <category>neurons</category>
        
        <category>Memory</category>
        
        
      </item>
    
      <item>
        <title>Using binary vector clocks to replace Ethereum transaction nonces</title>
        <description>&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;ol id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#contents&quot; id=&quot;markdown-toc-contents&quot;&gt;Contents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-order-of-ethereum-transactions&quot; id=&quot;markdown-toc-the-order-of-ethereum-transactions&quot;&gt;The order of Ethereum Transactions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#partial-orders-and-join-semilattices&quot; id=&quot;markdown-toc-partial-orders-and-join-semilattices&quot;&gt;Partial Orders and Join-Semilattices&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-binary-vector-clock&quot; id=&quot;markdown-toc-the-binary-vector-clock&quot;&gt;The Binary Vector Clock&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-inevitable-total-order-during-epoch-jumps&quot; id=&quot;markdown-toc-the-inevitable-total-order-during-epoch-jumps&quot;&gt;The inevitable total order during epoch jumps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;abstract&quot;&gt;abstract&lt;/h1&gt;
&lt;p&gt;In this blog post, I’ll present the idea of a Binary Vector Clock, a simple, yet space-efficient algorithm for generating a partial order of transactions in account-based blockchain systems. The Binary Vector Clock solves the problem of order dependency in systems such as Ethereum, caused by the total order of transactions that come from the same address holder. What that exactly means will become clear in a bit. The proposed algorithm has the same security as using regular transaction nonces, requires very little overhead, and can potentially result in a significant increase in throughput for systems like Ethereum. This paper was originally submitted on the 15th of April, 2020 on &lt;a href=&quot;https://arxiv.org/abs/2004.07087&quot;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;There are generally two kinds of transaction models in blockchains: The UTXO model, and the account based model. The UTXO model was the first transaction model to be proposed and has many intriguing properties. In this paper however, we are going to focus on the account based model, more exactly on the one implemented by Ethereum. In the account based model, instead of having coins as unspent outputs like in the UTXO model, every participating node has an account, or a balance. When a transaction is created, the transaction’s value is simply reduced from the owners account, and added to someone else’s account. To understand the problem that the Binary Vector Clock tries to solve, let’s first look at the structure of an Ethereum transaction and how the &lt;em&gt;order&lt;/em&gt; of transactions is determined.&lt;/p&gt;

&lt;h1 id=&quot;the-order-of-ethereum-transactions&quot;&gt;The order of Ethereum Transactions&lt;/h1&gt;
&lt;p&gt;A transaction in Ethereum is essentially a message that gets signed by an account holder, also known as an externally owned account. Once a transaction gets created, it is broadcast to other nodes in the system, and eventually recorded by the Ethereum blockchain. The structure of an Ethereum transaction consist of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;em&gt;value&lt;/em&gt; (the amount of ether we want to transfer).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;recipient&lt;/em&gt; (the address of the account to whom we want to send the transaction to).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;gas price&lt;/em&gt; (much like a transaction fee. The gas price shows how much of a fee the originator of the transaction is willing to pay).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;gas limit&lt;/em&gt; (the maximum fee that the originator is willing to pay).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;v,r,s&lt;/em&gt; (the three ECDSA digital signature components to prove that the originator truly formed the transaction).&lt;/li&gt;
  &lt;li&gt;A &lt;em&gt;data&lt;/em&gt; field (an optional field that can contain code, for when an account interacts with smart contracts).&lt;/li&gt;
  &lt;li&gt;And the &lt;em&gt;nonce&lt;/em&gt; (an account specific counter. Whenever a transaction from the address holder gets confirmed, the counter increments).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The nonce field is the field that is of particularly interest to us. The transaction nonce, not to be confused with the block nonce used for Proof of Work, is a scalar value that serves as a counter. The nonce shows the number of confirmed transactions that originated from the account. Having such a counter for each transaction has an interesting effect: It protects the user from transaction duplication. Let’s see what would happen if transactions had no nonce, to better understand why having such a counter is so important: Let’s say Alice sent Bob a completely valid transaction containing three ether. The signature turned out to be truly Alice’s, and the transaction got recorded on the blockchain. Bob however turns out to have a bad moral compass and wants more money. Without a transaction nonce, there is nothing to stop Bob from “replaying” Alice’s transaction, and claim again three ether. Bob could in fact repeat transmitting Alice’s old transaction to the network, until he gets all of Alice’s ether. Every time the transaction would be replayed, nodes in the system would think that it is a new transaction. In reality however, this is not what happens. By having a counter attached to the transaction, every transaction becomes unique. If let’s say Alice’s transaction has a nonce of 42, Bob will not be able to replay that transaction, as any new transaction coming from Alice would have to have a nonce greater than 42.&lt;/p&gt;

&lt;p&gt;There is however also another important reason to have a nonce in an account-based transaction: We want to be able to determine the &lt;em&gt;order&lt;/em&gt; of transactions. Let’s assume this time that Alice is sending two transactions, but the second transaction is dependent on the first one, i.e. running the second transaction before the first one is invalid (for whatever reason). In a centralized system this is no problem, one would simply confirm the first transaction first, and than continue with the second transaction. In a decentralized system however, nodes in the network might receive the second transaction before the first one. We cannot know in advance in which order nodes will perceive events. Without a counter, there would be no way for nodes in the network to tell which transaction comes first. If on the other hand the first transaction has a counter of 42 and the other transaction has the next counter (43), the order can be determined. If a node in the network thus receives the second transaction before the first one, it knows that it should ignore the second transaction, until the first transaction gets confirmed.&lt;/p&gt;

&lt;p&gt;This is a great feature, but it also has its shortcomings. If Alice were to send several transactions one after another, and one of the transactions does not get included in any block for some reason, e.g. the transaction turns out to be invalid, then none of the subsequent transactions get processed. Only after providing a transaction with the missing nonce, do all the other transactions get processed. 
This is no problem if every transaction depends on the previous one, but in most real-world applications that would not be the case. Many nodes have to create dozens of transactions in a short period of time, imposing an order dependency thus can result in transactions having to stay in mempools, even if they could have been processed sooner. The total order of transactions represents at the same time a great feature, and a serious scaling problem for account-based transaction models. In the following sections, I will present how we can overcome the problem of total order when processing transactions.&lt;/p&gt;

&lt;h1 id=&quot;partial-orders-and-join-semilattices&quot;&gt;Partial Orders and Join-Semilattices&lt;/h1&gt;
&lt;p&gt;Before jumping straight to how the Binary Vector Clock works, it is necessary to have a good grasp of what a partial order is. All of us intuitively understand the idea of “total orders” - One is smaller than two, five is greater than four, etc. In order theory, a set is said to have a total order, if for any element $a$ and $b$, a comparison is possible, i.e. either $a \leq b$ or $a \geq b$. For example: Every transaction nonce for an address, is comparable to any other transaction nonce for that address. We thus can easily know which transaction happened-before another transaction, thanks to the total order of transactions. But what if it does not matter in which order some of our transactions get confirmed? If eight out of ten transactions generated from an address holder could in fact be confirmed in any desired order, it would be quite wasteful not to do so. This is however what happens in today’s totally ordered account based transaction model.&lt;/p&gt;

&lt;p&gt;It would thus be of enormous interest if we could somehow “capture” the transaction independence for address holders. This is where partial orders become useful. A partially ordered set, is a set in which only certain pairs of elements are comparable, i.e. one element precedes the other in the ordering, but not every pair of elements is necessarily comparable.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot; height=&quot;300&quot; src=&quot;/assets/images/posts/2020/semilattice.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As an example to better understand what a partial order actually is, let’s look at the join-semilattice in figure 1. The diagram shows a set $S$ with eight vectors. We say that an element in $S$ ‘‘happened-before’’ another element, if and only if every value of vector $a$ is less than or equal to every corresponding value in vector $b$. For example: We can conclude that vector $(1,0,0)$ happened before vector $(1,1,0)$, because none of the values in vector $(1,0,0)$ are greater than in vector $(1,1,0)$ - We say that $(1,0,0)$ happened-before $(1,1,0)$. If on the other hand we try to compare vector $(1,1,0)$ and $(1,0,1)$, one can see that both vectors have values larger than the other vector at some indices. We say that this pair is &lt;em&gt;not comparable&lt;/em&gt;. One cannot determine which element occurred before the other one. Algorithms used in distributed systems, such as vector clocks, take advantage of partial orders. In the context of the distributed systems, having incomparable vectors, or “clocks”, usually means that the events occurred concurrently, and thus have no information of one another. In the case of the Binary Vector Clock on the other hand, incomparableness between two transactions does not indicate concurrency, it indicates that they occur &lt;em&gt;independently&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;the-binary-vector-clock&quot;&gt;The Binary Vector Clock&lt;/h1&gt;
&lt;p&gt;Let’s imagine that instead of a nonce (i.e. counter) for a transaction, we have a counter &lt;em&gt;and&lt;/em&gt; a very small bit array (for the sake of a better explanation, let’s stick to three bits, like the vectors in figure 1. Alice’s Binary Vector Clock is initially set to $(0, [0,0,0])$ (where the first element represents the counter and the second element the bit array). For simplicity, I will refer to the Binary Vector Clock from now on as a “timestamp”. Now let’s say Alice wants to send three transactions one after another. Alice however knows that her second transaction is dependent on her first transaction, but her third transaction has no logical dependency to the two first transactions. Having this information, Alice can do something clever: Instead of incrementing her counter for each transaction, she increments one of the bits in her bit array. Let’s say the first transaction has the timestamp $(0,[0,0,1])$, the second transaction has the timestamp  $(0,[0,1,1])$, and the third timestamp is $(0,[1,0,0])$. All three transactions were send one after another to the network. Any validator receiving the transactions can independently know in what order the transactions need to be confirmed (or if any order exists at all). Validators first look at the counter, the counter tells a validator if the transaction is in the right “epoch” (more on that in a bit). If the counter is equal to the previously confirmed transaction from that address, the bit array is checked. As the bit array of the first and third transaction are not comparable (no order can be determined), even if the first transaction turns out to be invalid for some reason, the third transaction can still be processed by the validators. This is because both timestamps are indicating “independentness”, there is no “happened-before” relationship between them. The second and the first transaction on the other hand do have a “happened-before” relationship. When looking at the bit array of the second transaction, we can conclude that it must have happened after the first transaction. If a validator thus would receive the third transaction and the second transaction, but not the first transaction for some reason, it would know that the third transaction can be processed, but the second transaction not, as it depends on a prior transaction (the first transaction). If a transaction gets confirmed, the address’ Binary Vector Clock gets simply added with the newly confirmed timestamp. Taking again the three transactions from the previous scenario as an example, if Alice’s initial timestamp was $(0,[0,0,0])$, and her first and third transactions get confirmed, her new timestamp would be $(0,[1,0,1])$. Once all the bits in the bit array are turned to one, we can increment the timestamp’s counter, and set the bit array to zero again. We call this shift an “epoch”.&lt;/p&gt;

&lt;p&gt;Up to this point, some of the readers might have already thought something in the lines of: But what if Alice has only one ether, and she creates three independent transactions, each spending one ether? It is important to remember that this is an issue only if transactions would be processed concurrently, which is not the case with the Binary Vector Clock technique. In cases like the one mentioned above, transactions would be treated the same way today’s transactions get treated, if they were to have the same nonce. Today, with the nonce approach, if transactions have the same nonce, one of the transactions would get confirmed (depending on the block creator) and the rest of the transactions would become invalid. In the case of the Binary Clock, one of Alice’s transactions (depending on the block creator) would get confirmed, while the rest of the transactions would simply be considered invalid, regardless of their order independency.&lt;/p&gt;

&lt;h1 id=&quot;the-inevitable-total-order-during-epoch-jumps&quot;&gt;The inevitable total order during epoch jumps&lt;/h1&gt;
&lt;p&gt;It is important to note that there is nonetheless an inevitable transaction processing dependency when shifting from one epoch to the next. Transactions from one epoch can only be processed independently, after the transactions of the previous epoch were already processed. In other words if Alice were to send three other transactions one after the other, where the first transaction would have a timestamp of $(0,[1,1,1])$, second transaction $(1,[1,0,0])$, and third transaction $(1,[0,0,1])$, even if all three transactions are completely independent from one another, the second and third transactions will not be able to get processed without the first one being confirmed first. This is because these transactions occurred during an epoch “jump”, i.e. the Binary Vector Clock gets incremented, and the bit array becomes set to zero. The transactions in the new epoch cannot know if they are comparable or not with the transaction from the previous epoch, forcing a momentary total order. I argue however that the space-wise inexpensive nature of the Binary Vector Clock, and its property to handle partial orders, makes it an attractive technique for the account-based transaction model, even in the case of momentary order dependencies between epoch jumps.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In This paper I introduced the Binary Vector Clock, a memory-wise inexpensive partially ordered counter for account-based transactions, that solves the issue of order dependency when processing transactions. Note that the Binary Vector Clock does not suggest the concurrent processing of transactions in Ethereum. Doing so would in fact introduce many possible attack vectors to the system. It only specifies which transactions can be processed independently, and which ones depend on a prior transaction confirmation. If for example an address generates $N$ transactions one after another, and the first transaction fails, the subsequent transactions are still able to get processed and confirmed by the blockchain. This is not the case in today’s approach with transaction nonces. In today’s approach, if the first transaction fails for some reason, all of the other transactions would need to be ignored until the gap in the nonce becomes filled. The Binary Vector Clock overcomes the issue by introducing a partial order between transactions of the same address holder. Using the Binary Vector Clock as a substitution for the transaction nonce gives more freedom to the user in determining transaction orders. The Binary Vector Clock allows the user to specify if a transaction can be processed  independently from other transactions, or if it should be queued until a certain transaction gets confirmed. I argue that this ability has important implications for blockchain systems. Considering that transactions in blockchain systems most likely follow a pareto distribution (the majority of transactions are generated by very few nodes), introducing an inexpensive technique that allows for independent processing of transactions, could potentially increase the scaling capability of Ethereum and other account-based blockchains significantly.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 May 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/The-Binary-Vector-Clock</link>
        <guid isPermaLink="true">http://localhost:4000/The-Binary-Vector-Clock</guid>
        
        <category>Paper</category>
        
        <category>Vector Clock</category>
        
        <category>Ethereum</category>
        
        
      </item>
    
  </channel>
</rss>
